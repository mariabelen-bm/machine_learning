---
title: "Entrega 1. MACHINE LEARNING [21/22]"
subtitle: "Máster en Bioinformática. Universidad de Murcia"
author: "María Belén Barquero Martínez"
linkcolor: blue
urlcolor: blue
date: '\today'
bibliography: bibliografia.bib
output: 
  html_document:
    fig_caption: yes
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

# Introducción
En genética se denomina **región no traducida** o **UTR** a los sectores extremos de los genes. Generalmente se habla de dos extremos: **región no traducida cinco prima** (**5'-UTR**) y **región no traducida tres prima** (**3'-UTR**). Estas dos regiones están presentes en los genes y corresponden a las dos partes no traducidas de estos, debido a que se encuentran en los extremos de los marcos abiertos de lectura u ORFs, formados por exones e intrones, como se puede apreciar en la Figura expuesta a continuación, que muestra el paso del RNA inmaduro (primera figura) a maduro (segunda figura).

![Paso de mRNA inmaduro a mRNA maduro](figura1.png)

Pese a que estos extremos son las partes no codificantes de los mRNA, las regiones 3'-UTR poseen una gran importancia en la regulación de la expresión génica, ya que se sabe que estas regiones determinan el destino de las proteínas mediante la regulación de las interacciones proteína-proteína [@mayr2017regulation]. Es conocido que hay proteínas adaptadoras que reconocen secuencias específicas no codificantes del 3'-UTR, así como también que estas regiones están implicadas en la correcta expresión espacial y temporal de los genes. 

También se sabe que estas secuencias 3'-UTR  facilitan la formación de complejos proteicos cotraduccionales [@mayr2017regulation], lo que establece un papel para los 3'-UTR como **operones eucariotas evolucionados**. Mientras que los operones bacterianos promueven la interacción de dos subunidades, las 3'-UTR permiten la formación de complejos proteicos con composiciones diversas.

En general, **las regiones 3'-UTR parecen ser importantes en la regulación de genes que involucran funciones locales, compartimentación y cooperatividad**, lo que las convierte en herramientas importantes para la regulación de la diversidad fenotípica de organismos superiores [@mayr2017regulation]. Saber identificar estas regiones puede resultar beneficioso para entender mejor cómo funcionan determinados genes de los que todavía no se sabe mucho sobre su modo de actuación y para el estudio de enfermedades, ya que se sabe que, por ejemplo, la estabilidad del mRNA mediado por la región 3' no traducida (3'-UTR) desempeña un papel crucial en el cambio de la expresión génica en la fisiopatología cardiovascular [@misquitta2001role].

En este trabajado se va a intentar, mediante técnicas básicas de estadística para asociación de variables y técnicas de *machine learning* para elaboración de modelos, **identificar regiones 3'-UTRs en el genoma humano**. 

# Objetivos 

El objetivo principal del estudio a partir de los datos que se van a tratar es lograr saber **si es posible distinguir 3'UTRs de entre cualquier tipo de región que se pueda introducir en el modelo a partir de su especificación mediante las 41 características genómicas y transcriptómicas que presentan los datos**.

# Metodología de trabajo

Este estudio se va a dividir en dos partes: 

* La **primera parte** del estudio va a corresponder a la **preparación de datos**. Se va a proceder a la **organización**, **descripción de los datos** y **selección de variables**, con el objetivo de seleccionar solo aquellas que sean relevantes para el estudio. El objetivo de esta entrega es preparar los datos para la segunda parte del estudio.

* La **segunda parte** del estudio corresponde a la **obtención de los modelos de machine learning que van a hacer posible en principio responder a la pregunta de si es posible identificar 3’ UTRS en el genoma humano**.

PRIMERA PARTE

# Pasos previos

## Espacio de trabajo
Para llevar a cabo la realización de este estudio, lo primero que se va a hacer es especificar el espacio de trabajo. El trabajo se va a realizar en dayhoff, el clúster que se encuentra a disposición de los estudiantes del máster en bioinformática de la Universidad de Murcia, creando para ello un directorio llamado `entrega1` para alojar este archivo R.Markdown así como los datos utilizados en el estudio y los archivos rds que se vayan generando.

```{r}
set.seed(123)
# Espacio de trabajo
setwd("/home/alumno02/entrega1")
```

```{r, warning=FALSE, cache=FALSE, include=FALSE}

.libPaths("/home/alumno02/R/x86_64-pc-linux-gnu-library/4.0")

# Librerías necesarias para el desarrollo del trabajo
library(knitr)
library(caret)
library(dplyr)
library(kableExtra)
library(ggcorrplot)
library(corrplot)
library(caret)
library(psych)
library(MASS)
library(tidyverse)
library(foreach)
library(iterators)
library(doParallel)
library(DT)
library(Epi)
library(nortest)
library(gridExtra)
library(FactoMineR)
library(factoextra)
library(umap)
library(venn)
library(ggplot2)
library(reshape)
library(foreach)
library(iterators)
library(doParallel)
library(DT)
library(Epi)
library(nortest)
library(lubridate)
library(keras)
library(tensorflow)
library(reticulate)
library(partykit)
library(rattle)
library(pROC)
library(coin)
library(PMCMRplus)
library(ez)
library(car)

knitr.table.format = "html"
```

## Obtención de los datos
Los datos para la realización de este estudio se han obtenido a través del aula virtual de la asignatura bajo el nombre de `dataPracticaML21_22.rds`, pero su extracción se realizó previamente del artículo [@sethi2021leveraging], al que se puede acceder a través de esta [url](https://www.biorxiv.org/content/10.1101/2021.03.08.434412v1.full).

Estos datos van a ser cargados y asociados al nombre de una variable.

```{r}
# Asociación del fichero de datos 'dataPracticaML21_22.rds' a la variable 'data'
data <- readRDS("dataPracticaML21_22.rds")
```

# Desarrollo - Primera entrega

En esta primera entrega se va a proceder a realizar un **análisis exploratorio de los datos** con el fin de comprobar algunas propiedades respecto a ellos. A continuación, se muestra una lista con las principales incógnitas a resolver en este apartado:

 * Se comprobará la **existencia de valores nulos en los datos**, así como otros tipos de valores no válidos para el estudio, y en el caso de detectar su presencia, se procederá a ver cómo se tratan.
 * Se observará del **tipo de datos del que se dispone y si los datos se encuentran balenceados o no**, es decir: si se encuentra presencia de clases mayoritarias y minoritarias, y se verá cómo tratar el problema dependiendo de esto.
 * Se comprobará la **existencia o no de correlación entre predictores**, y qué utilidad tiene esto en el estudio.
 * Se estudiará la **presencia de asociaciones lineales entre los distintos predictores y alguno de los tipos de regiones presentes ('class')**.
 * Se estudiará también si se distribuyen con **normalidad** aquellas variables que a priori se identifiquen como interesantes por su relación con la variable de clase y qué dice un **análisis no supervisado** de la separabilidad de las regiones presentes en los datos.
 
Tras esto, se va a proceder a realizar una **selección de variables** con el objetivo de dejar los datos preparados para la segunda entrega, que consta de la obtención de los modelos de machine learning que permitan identificar refiones 3'-UTR.
 
## Información de naturaleza exploratoria de los datos

### Exploración principal de los datos

Si se analiza el frame de datos del que se dispone se podrá comprobar que es un archivo rds compuesto por 179968 filas y por 42 columnas. Si se procede a la visualización los datos se observará que de esas 42 columnas:

 * 41 son predictores.
 * 1, una variable de clase. 

Dicha variable de clase corresponde a la última columna, de nombre 'class', y hace referencia a la región que corresponde a cada uno de los datos, pudiendo ser dichas clases/regiones: 5’UTRs, 3’UTRs, *long non coding RNAs*, *non coding RNAs*, pseudogenes e *internal coding exons*. 

A continuación se comprobará que las variables tienen asignados correctamente el tipo de dato al que pertenecen. 

Este dataset cuenta con **40 variables numéricas y 2 factores**. Uno de los factores es la variable de clase de la que se ha hablado anteriormente, y la otra es la variable 'polyA_signal', que tiene dos niveles (0, 1), y que determinan la no presencia o presencia del mismo respectivamente.

A continuación se visualirazará el número de datos asociados a cada tipo:

```{r}
# Exploración principal de los datos
str(data)

# Número de filas y columnas de los datos
dim(data)
```

### Escalado de los datos

A continuación se va a proceder a la comprobación de la **normalización y/o escalado de los datos**, es decir, se va a comprobar si los datos están en un mismo rango. Para ello se van a seleccionar las columnas [2:41], que son las numércias, y se va a realizar con ellas un boxplot:

```{r, warning=FALSE}
# Boxplot de las variables numéricas
  ## Creación de una estructura de datos para plotear con facilidad filtrando sólo las variables numéricas
mymelt <- melt.data.frame(data[,2:42], id = "class")

  ## Representación con ggplot
ggplot(data=mymelt, aes(x=variable, y=value, color=variable, fill=variable)) +
  geom_boxplot(alpha=0.6) +
  theme_bw() +
  scale_fill_discrete() +
  scale_color_discrete() +
  ggtitle("Diagramas de cajas de los datos brutos") +
  ylab("Valores") +
  xlab("Variables") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(text = element_text(size=6)) +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
```

Como se puede comprobar en el boxplot anterior, **hay algunas variables que no están escaladas**, como por ejemplo, la variable 'meanPd', que es la primera a la izquierda del todo. También se observa que, siguiende un orden de izquierda a derecha, las variables 6, 8, 12, y 18 tampoco están escaladas.

Queda justificada entonces la **realización de un proceso de escalado y centrado de datos**. La normalización va a transformar las características de forma que todas compartan un mismo valor medio y una misma desviación media.

Para escalar y centrar las variables numéricas se va usar la función `preProcess` de `Caret`, función que se puede usar para muchas operaciones con predictores, incluidas las de centrado y escalado. 

Tras esto, se volverán a representar las variables en un boxplot para comprobar si se ha producido de forma satisfactoria este proceso de escalado y centrado.

```{r, warning=FALSE}
# Boxplot de las variables numéricas escaladas y centradas
  ## Se selecionan las variables numéricas y se escalan y centran los datos
escalado_y_centrado <- preProcess(data[,2:41], method = c("center", "scale"))
data_escalados <- predict(escalado_y_centrado,data[,2:41])
my_melt_escalados <- melt.data.frame(data_escalados)

  ## Representación con ggplot
ggplot(data=my_melt_escalados, aes(x=variable, y=value, color=variable, fill=variable)) +
  geom_boxplot(alpha=0.6) +
  theme_bw() +
  scale_fill_discrete() +
  scale_color_discrete() +
  ggtitle("Diagramas de cajas de los datos escalados y centrados") +
  ylab("Valores") +
  xlab("Variables") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(text = element_text(size=6)) +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
```

Como se puede comprobar, **las variables numéricas se han escalado y centrado**, pudiendo ser usadas ahora en futuros análisis del estudio.

Se puede comprobar la presencia de valores atípicos en el caso de variables como 'meanEE' (*outliers* en valores negativos).

```{r}
data_escalado_numericas = readRDS("./data_escalados_variables_numericas.RDS")

# Se añaden las variables factoriales
data_escalado_completo <- cbind("polyA_signal"= data[,1], data_escalado_numericas,"class" = data[,42])
data_escalado_completo = readRDS("./data_escalado_completo.RDS")
```


### Comprobación de valores nulos

Se procederá a la comprobación de si **todos los valores son adecuados o si hay algún valor con el que no se pueda trabajar** en los análisis posteriores. Los tipos de datos no válidos son: NA (no disponibles), NaN (valores desconocidos), Inf (valores infinitos) o NULL (valores nulos):

```{r, warning=FALSE}
no_validas <- table(apply(data_escalado_completo, 2, function(x) any(is.na(x) | is.nan(x) | is.infinite(x) | is.null(x))))

# Tabla de las columnas con valores no aptos para el estudio
knitr::kable(no_validas, caption = "Columnas con valores no numéricos o ausentes",  col.names = c("Tipo", "Frecuencia")) %>%
  kable_styling(bootstrap_options = "condensed")
```

Como se puede apreciar, ninguna de las columnas ha dado positivo en el test que comprueba la presencia de cualquiera de estos cuatro tipos de valores, por lo que se concreta la **no presencia de valores no adecuados en los datos** que se utilizan en este estudio.

### Tipo de datos y presencia de balanceo en los datos

Ahora se procederá a estudiar el **tipo de datos del que se dispone**, así como el **número de cada una de las clases** que hay presentes en los datos datos. 

Como se puede apreciar a continuación, todas las columnas del archivo 'data' poseen el tipo de dato 'Numeric', mientras que se aprecia 2 únicas columnas que tienen como tipo de datos 'Factor', y estas columnas corresponden a la de 'class' y 'polyA_signal' si se visualiza el archivo de los datos. 

No se va a analizar el factor 'polyA_signal' porque no aporta ningún tipo de información más allá de la presencia o no del mismo, por lo que no se va a usar para medir el balanceo/desbalanceo de los datos.

Se procederá ahora a la **comprobación del desbalanceo en los datos**. Para ello hay que fijarse en la clasificación de las variables dependiendo de la clase en la que están:

 * Si se observa la tabla que se forma como resultado de agrupar los datos por su variable de clase, así como en el barplot posterior, se puede comprobar el número de ellos que se encasillan en cada clase, comprobando la presencia de clases mayoritarias y minoritarias, como por ejemplo, "ICE", que presenta una frecuencia de 130768, y siendo esta la clase más mayoritaria, agrupándose en ella el 72% de las obvservaciones. Por el contrario, se cuenta solamente con 2146 datos agrupados en la clase de "pseudoGene" (0.12%), por lo que se puede concluir que claramente **hay un desbalanceo en los datos, ya que hay una serie de clases mayoritarias y otras que son bastante minoritarias**.
 

```{r}
# Clases del conjuntos de datos
tipos <- table(sapply(data_escalado_completo, class))
knitr::kable(tipos, caption = "Clase de datos de nuestro conjunto de datos", col.names = c("Tipo de dato", "Frecuencia")) %>%
  kable_styling(bootstrap_options = "condensed")

# Tabla con las clases presentes en los datos y el número de cada una de ellas
tabla_clases <- table(data_escalado_completo$class)
knitr::kable(tabla_clases, caption = "Número de datos agrupados por cada clase", col.names = c("Clases", "Frecuencia")) %>%
  kable_styling(bootstrap_options = "condensed")

# Frecuencia relativa de cada una de las clases 
prop_clase <- prop.table(table(data_escalado_completo$class))
knitr::kable(prop_clase, caption = "Proporción de cada clase", col.names = c("Clases", "Frecuencia relativa")) %>%
  kable_styling(bootstrap_options = "condensed")

# Barplots con proporción de clases
barplot(prop_clase, col = palette("Pastel 2"), ylab ="Frecuencias Relativa", ylim = c(0, 1), main = "Distribución de Clases")
```

Este problema de desbalanceo en los datos va a hacer que en el conjunto de datos de entrenamiento se cuente con clases “minoritaria”, de las cuales contamos con menos muestras que en otras clases que se entienden como "mayoritarias". Esto va a provocar un desbalanceo en los datos que se utilizan para el entrenamiento. Este desbalanceo afecta a los algoritmos en su proceso de generalización de la información y perjudica a las clases minoritarias.

Las estrategias para el manejo de datos desbalanceados se puede resumir en:

* **Ajuste de Parámetros del modelo**: Consiste en ajustar parametros o métricas del propio algoritmo para intentar equilibrar a la clase minoritaria penalizando a la clase mayoritaria durante el entrenamiento. No se puede usar en todos los algoritmos.
* **Modificar el Dataset:**: También se pueden eliminar muestras de la clase mayoritaria para reducirla e intentar equilibrar. Tiene como peligro el poder prescindir de muestras importantes, que brindan información y por lo tanto empeoran el modelo. También se podría agregar nuevas filas con los mismos valores de las clases minoritarias, pero esto en ocasiones no sirve demasiado y además podría llevar a nuestro modelo a caer en *overfitting*.
* **Muestras artificiales**: Se puede intentar crear muestras sintéticas (no idénticas) utilizando diversos algoritmos que intenten seguir la tendencia del o de los grupos grupo minoritario. Según el método que utilicemos podemos mejorar los resultados o no. El peligro de este método es que se puede ver alterada la distribución normal de esa clase y confundir al modelo en su clasificación.
* ***Balanced Ensemble Methods***: Consiste en entrenar diversos modelos y entre todos obtener el resultado final, pero se asegura de tomar muestras de entrenamiento equilibradas. 

En este caso, la forma de proceder sería realizar una evaluación del modelo sin balancear y posteriormente, evaluar el modelo tras balancearlo con técnicas como la de *oversampling* o la penalizando clases mayoritarias, con el fin de comparar qué modelo da mejores resultados para utilizar este. 

Este procedimiento se llevará a cabo en la segunda parte del estudio, antes de la realización de los modelos.

## Análisis de correlación

### Correlación de Spearman

Para comprobar la relación entre las distintas variables presentes en los datos se va a proceder al estudio de la **correlación linear** entre las mismas. Lo primero que se hará para ello será realizar un plot de _correlación de Spearman_ entre todas las variables, organizándolas en función de un algoritmo de clustering que permita analizar los distintos grupos de variables que hay.

Además de este gráfico se presenta el resultado del _test estadístico de Bartlett_. El resultado de este test indica si la matriz de correlación es significativamente diferente a la matriz identidad, es decir, si hay correlación entre las distintas variables presentes en los datos datos.


```{r}
# Tratamiento previo y filtrado de los datos
  ## Se eliminan los valores con cero varianza y se preparan los datos de las variables numéricas
no_zero_var <- nearZeroVar(data_escalado_numericas)
datos_filtrados <- data_escalado_numericas[,-no_zero_var]
datos_filtrados_frame <- cbind(as.data.frame(datos_filtrados), data[42])
  ## Filtrado en dataset completo
nzv <- nzv(data_escalado_completo)
data_escalado_completo_nzv = data_escalado_completo[, -nzv]

# Test de Spearman
spearman <- cor(datos_filtrados , method = "spearman")

# Estudio de la correlación lineal con corplot
  # Se calcula una matriz de valores p de correlación
p.mat <- cor_pmat(as.data.frame(spearman))

  # Corplot con ggplot
ggcorrplot(as.data.frame(spearman), hc.order = TRUE, outline.col = "white", colors = c("#6D9EC1", "white", "#E46726"), title="Análisis de correlación - Spearman", legend.title="Leyenda de corrplot", tl.cex=4, p.mat = p.mat, insig = "blank")
```
```{r}
data_escalado_completo_nzv <- readRDS("./data_escalado_completo_nzv.RDS")
```

En el análisis de correlación presentado con anterioridad se pueden apreciar en color blanco los coeficientes que no son significativos, mientras que los que se aprecian coloreados y poseen una tonalidad más fuerte de color, son los coeficientes más significativos. Cabe destacar que una correlación, por pequeña que sea, si es significativa y se debe considerar.

En el caso de la **tonalidad anaranjada haría referencia a las correlaciones positivas, mientras que las tonalidades azules, a las negativas**.

Lo que se puede observar es una **correlación entre las variables que describen propiedades topológicas**, como es el caso de 'aphilicity' y 'zdna', y a su vez, estas propiedades poseen correlacionadas altas con la aparición de algunos grupos de dos nucleótidos o nucleótidos individuales, como 'T', 'TT', 'TA'...

```{r}
# Cortest Bartlett
psych::cortest.bartlett(spearman,42)
```
Como se ha mencionado anteriormente, el _test de Bartlett_, cuyos resultados se muestran en el paso anterior, indica si hay una diferencia significativa con la matriz identidad. Al tener como hipótesis nula la igualdad entre ambas matrices, un _pvalor_ como el presente de 6.14463e-212, mucho menor que 0.05, obliga a rechazar esta hipótesis nula y a asumir que hay diferencias entre ambas matrices, es decir, que **hay variables que están significativamente correlacionadas**.

Tras esto se va proceder a visualizar la siguiente gráfica que va a permitir evaluar las **relaciones verdaderas entre cada predictor y la variable respuesta de forma independiente**. Este test va a ayudar a aclarar cualquier duda acerca de la influencia de las variables (eje X) con el valor de y:

```{r, warning=FALSE}
# Creación de la matriz de cor
cor_matriz = cor(data_escalado_numericas)

# Aplicación de cor.test sobre cada columna con la variable respuesta, guardando el p-valor
formacion_matriz = function(cor_matriz, p_values){
  upper=upper.tri(cor_matriz)
  data.frame(row=rownames(cor_matriz)[row(cor_matriz)[upper]],
           column=rownames(cor_matriz)[col(cor_matriz)[upper]],
           p=p_values[upper])}

# Asignación del nombre 
matriz_datos = formacion_matriz(cor_matriz,cor_matriz)

# Representación con ggplot
ggplot(data=matriz_datos) + aes(x=row,y=-log10(p), fill=row) +
  geom_col() + geom_hline(yintercept = -log10(0.05), col="black", linetype="dotted") +
  theme_bw() +
  theme (text = element_text(size=6)) +
  labs(title="-log10(pval) para correlaciones predictores vs y",
       ylab="p-valor)",xlab="Predictores", face="bold") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
```

Hay que recordar que H0 indica que NO hay correlación entre las variables, o lo que es lo mismo: que las parejas de variables para las cuales el p-valor sea MENOR de 0.05 (valor que se indica en la línea de la gráfica anterior) permitirán RECHAZAR esa hipótesis nula con una confianza del 95% para afirmar que existe asociación entre ellas con casi total probabilidad. Cuanto más pequeño sea el valor, más significativa será dicha relación. 

Como se realizó el -10(pvalor), las conclusiones que se obtendrían serían al contrario, y por tanto, lo que se puede concretar es que en el modelo, **las variables CA, CT, GA y TA NO están correlacionadas, mientras que el resto de variables sí lo están**.

### Eliminación de variables muy correlacionadas
Para futuros análisis puede ser necesario **trabajar con variables que no estén altamente correlacionadas**. Es por esto que se va a hacer uso de la función `findCorrelation` de `Caret`, la cual identifica variables con una alta correlación entre sí y tras esto calcula la correlación absoluta media de cada una de estas con el resto de variantes. Posteriormente a esto, se puede proceder a la eliminación de estas variables.

El criterio de _cutoff_ o punto de corte para medir qué correlaciones se van a eliminar queda a criterio del autor y a las necesidades que presente el análisis. En este caso, se decidió eliminar aquellas variables que presentaran una correlación superior a 0.90 con el objetivo de no eliminar muchas variables que pudieran ser predictores útiles en futuros análisis.

```{r}
# Correlaciones altas de Spearman
find_cor_spearman <- caret::findCorrelation(
  spearman,
  cutoff = 0.9,
  verbose = TRUE,
  names = TRUE)

find_cor_spearman
```

La función `findCorrelation`, en el caso de usar la _cor de Spearman_, arroja los resultados mostrados con anterioridad. Estas variables poseen una correlación mayor a 0.90, y podrían aportar información redundante.

Como se puede comprobar, estas variables corresponden a propiedades topológicas y también termodinámicas que se presentan como muy correlacionadas en la representación de Spearman.

Previamente se ha procedido a la eliminación de variables con varianza cercana a 0. Esto también evita incluir información redundante en el modelo y puede mejorar la precisión del modelo. Además de esto, se va a proceder a **la elminación de de variables altamente de correlacionadas** según la información arrojada con anterioridad gracias a la ayuda de `findCorrelation`:

```{r, all_of}
# Eliminación de variables correlacionadas
  ## Númericos
datos_filtados_cor <- dplyr::select(datos_filtrados, -all_of(find_cor_spearman))
  # Numéricos y factores
datos_filtradosyescalados_cor_completos <- dplyr::select(data_escalado_completo_nzv, -all_of(find_cor_spearman))
```

```{r}
# Numéricos
datos_filtados_cor <- readRDS("./datos_filtados_cor.RDS")

# Numéricos y factores
datos_filtradosyescalados_cor_completos <- readRDS("./datos_filtradosyescalados_cor_completos.RDS")

# Numérico + 'class' pero sin 'polyA_signal'
datos_completos_escalados_cor_sinpolyA <- datos_filtradosyescalados_cor_completos[2:30]
datos_completos_escalados_cor_sinpolyA <- readRDS("./datos_completos_escalados_cor_sinpolyA.RDS")
```

## Modelo de regresión

En este apartado **se va a intentar encontrar asociaciones lineales entre los predictores numéricos o variables numéricas (valores continuos) y la variable de clase**, que es un factor discreto. 

Para realizar esta tarea se van a implementar modelos de **regresión logística binaria**, ya que esta es ideal para los casos en los que se desea predecir la presencia o ausencia de una característica o resultado según los valores de un conjunto de predictores. Esta regresión logística es buena para determinar asociaciones entre predictores continuos y variables respuesta binarias. Para evaluar qué tan bien se ajusta un modelo de regresión logística a un conjunto de datos, se pueden observar dos métricas:

 * Sensibilidad: es la probabilidad de que el modelo prediga un resultado positivo para una observación cuando en realidad el resultado es positivo.
 * Especificidad: es la probabilidad de que el modelo prediga un resultado negativo para una observación cuando en realidad el resultado es negativo.

Una forma fácil de visualizar esas dos métricas es crear una **curva ROC**, una gráfica que muestra la sensibilidad y especificidad de un modelo de regresión logística. Se verá al final del apartado.

Se va a proceder a crear, primero, un modelo que distinga las secuencias 5'-UTR de las que no lo sean, y así con el resto de clases: ICE, lncRNA, ncRNA, pseudoGene y 3'-UTR, respectivamente. Tras esto, se va a comprobar las variables más significativas en los distintos modelos de regresión que se han creado.

### Regresión logística binaria de 5'-UTR

```{r}
# Modelo 5-UTR
modelo_5UTR = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "5-UTR" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_5UTR)
```
En el modelo expuesto con anterioridad se pueden distinguir las secuencias 5'-UTR de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que **todas las variables son significativas para esta clase 5-UTR**, lo que indica que hay una relación lineal entre ellas y la región 5-UTR.

Es la única clase en la que todas las variables expuestas hasta el momento se muestran significativas.

### Regresión logística binaria de ICE
```{r}
# Modelo ICE
modelo_ICE = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "ICE" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_ICE)
```
En el modelo expuesto con anterioridad se pueden distinguir las secuencias ICE de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que variables como por ejemplo **'bdnatwist', 'bendability', 'proteindnatwist', 'AT', 'TA', 'TC' y 'GA' NO son significativas para esta clase ICE**. El resto de variables sí que muestran significancia, lo que indica que hay una relación lineal entre ellas e ICE.

### Regresión logística binaria de lncRNA
```{r}
# Modelo lncRNA
modelo_lncRNA = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "lncRNA" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_lncRNA)
```
En el modelo expuesto con anterioridad se pueden distinguir las secuencias lncRNA de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que variables como por ejemplo **'bdnatwist', 'bendability', 'G', 'C', 'TT', 'GG', 'AT', 'AG', 'TC', 'GA' y 'CT' NO son significativas para esta clase lncRNA**. El resto de variables sí que muestran significancia, lo que indica que hay una relación lineal entre ellas y lncRNA.

### Regresión logística binaria de ncRNA
```{r}
# Modelo ncRNA
modelo_ncRNA = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "ncRNA" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_ncRNA)
```
En el modelo expuesto con anterioridad se pueden distinguir las secuencias ncRNA de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que variables como por ejemplo **'AA' y 'GG' NO son significativas para esta clase ncRNA**. El resto de variables sí que muestran significancia, lo que indica que hay una relación lineal entre ellas y ncRNA.

### Regresión logística binaria pseudoGene
```{r}
# Modelo pseudoGene
modelo_pseudoGene = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "pseudoGene" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_pseudoGene)
```
En el modelo expuesto con anterioridad se pueden distinguir las secuencias pseudoGene de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que variables como por ejemplo **'meanEE', 'aphilicity', 'bdnatwist', 'bendability', 'cpnpgislands', 'proteindnatwist', 'T', 'AA', 'TT', 'TC' y 'GA' NO son significativas para esta clase pseudoGene**. El resto de variables sí que muestran significancia, lo que indica que hay una relación lineal entre ellas y pseudoGene.

Esta clase en particular es la que **menos relaciones lineales muestran con el total de variables**.

### Regresión logística binaria UTR
```{r}
# Modelo UTR
modelo_UTR = glm(formula = datos_completos_escalados_cor_sinpolyA$class == "UTR" ~ ., family = binomial(link = "logit"), 
    data = datos_completos_escalados_cor_sinpolyA)
summary(modelo_UTR)
```

En el modelo expuesto con anterioridad se pueden distinguir las secuencias 3'-UTR de las que no lo son. Se puede observar que al enfrentar esta clase con las variables se obtiene que la variable **'cpnpgislands' NO es significativas para esta clase 3'-UTR**, siendo la única que no lo es, y por tanto, el resto de variables sí que muestran significancia, lo que indica que hay una relación lineal entre ellas y 3'-UTR.

Como **conclusiones** del procedimiento anterior, destacar:

 * Como se ha mencionado, **pseudoGene es la variable de clase que menos relaciones lineales muestran con el total de variables**, pero sin embargo, presenta algunas variables de alto nivel de significancia que sí están presentes en todos los modelos, como 'meanPd' o 'mean_phastCons7way'.

 * En el caso del modelo logístico binomial que distingue 3'-UTR de no 3'-UTR, se observan relaciones lineales muy significativas con todas las variables menos con "cpnpgislands", variable que sí se presenta significativa en otros modelos. Esto habrá que tenerlo en cuenta posteriormente, ya que el objetivo del estudio es predecir 3-UTR de las regiones que no lo son, y esta característica puede resultar interesante.
 
 * En el caso del modelo logístico binomial que distingue 5'-UTR de no 5'-UTR se puede observar que presenta relaciones lineales muy significativas con TODAS las variables.

 * Se observa que las variables que no son significativas en un modelo lo son en otro. 

 * Se encuentran variables como "meanPd" y meanEE" (relacionadas con datos transcriptómico) que son significativas en casi todos los modelos, menos en el caso de pseudoGene, en la que esta variable 'meanEE' no es significativa, siendo la única clase en la que se da esto.  

Se puede decir que las variables escogidas hasta ahora parecen adecuadas para continuar con el análisis.

### Chi-cuadrado para poly-A

La variable poly-A es factorial, es decir, no es numérica y su contenido se basa en detectar la presencia de la esta poly-A mediante el valor 1, y la no presencia mediante el valor 0. 

A continuación se va a proceder a comprobar la presencia o no de esta variable poly-A dependiendo de la clase:

```{r}
polyA <- table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class)

knitr::kable(polyA , caption = "Distribución Poly-A") %>% kable_styling(bootstrap_options = "condensed")
```


Como se puede apreciar en la tabla anterior, **en todas las clases lo que predomina es que NO esté presente esta Poly-A, menos en el caso de 3'UTR, en la que SÍ predomina la presencia de esta variable factorial frente a la no presencia de la misma**. Esto tiene sentido, ya que desde el punto de vista biológico la cola Poly-A se encuentra a continuación del extremo 3-UTR. 

Se puede comprobar esta asociación desde un punto de vista estadístico mediante una **prueba chi-cuadrado para relacionar variables discretas**, como se muestra a continuación para todas las clases:

```{r}
# Relación de poly-A con la clase 3'-UTR
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "UTR"))
```
```{r}
# Relación de poly-A con la clase 5'-UTR
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "5-UTR"))
```
```{r}
# Relación de poly-A con la clase ICE
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "ICE"))
```
```{r}
# Relación de poly-A con la clase lncRNA
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "lncRNA"))
```
```{r}
# Relación de poly-A con la clase ncRNA
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "ncRNA"))
```
```{r}
# Relación de poly-A con la clase pseudoGene
chisq.test(table(datos_filtradosyescalados_cor_completos$polyA_signal, datos_filtradosyescalados_cor_completos$class == "pseudoGene"))
```

Una vez realizadas las pruebas, se observa que los datos de poly-A se relacionan con las distintas clases de forma significativa. 

Como se puede comprobar en la primera tabla, su presencia está muy relacionada con la presencia de la región 3’-UTR.

### Curvas ROC

Una vez comprobados los modelos de regresión que distinguen por clase, se puede probar a introducir el modelo con todas las variables vistas en las regresiones y comprobar estos **predictores significativos y la probabilidad asociada a que su presencia o ausencia determine la clase a la que pertenecen los datos de los que disponemos**. Una pregunta que se podría hacer es el cómo se determina dónde está la línea que marca la probabilidad. Dónde estaría ese punto de corte. 
Para determinar este punto de corte se podría utilizar un **análisis de curvas ROC**.

Las curvas ROC (característica operativa del receptor) constituyen una herramienta importante para evaluar el rendimiento de un modelo de machine learning. Por lo general, se utilizan en problemas de clasificación binaria, concretamente, problemas con dos clases de salidas distintas [@park2004receiver].

En esta ocasión, se va a utilizar la función `ROC` del paquete `Epi` para lograr dicho objetivo.

Las **curvas ROC arrojan información de su capacidad a la hora de discriminar en modelos de clasificación**. En concreto, el área que hay bajo esta curva indica la calidad del modelo, siendo un **modelo útil a partir de áreas superiores a 0.7**, con la particularidad de que esa área aporta el valor del **punto de corte óptimo**. Este punto de corte óptimo coincide con el codo de la curva, es decir: el punto en el que se "pliega".

```{r}
# Análisis de curvas ROC del modelo completo
datos_roc <- ROC(form = class ~ meanPd + meanEE + mean_phastCons7way + aphilicity + bdnatwist + bdnatwist + bendability + cpnpgislands + proteindeformation + proteindnatwist + T + G + C + AA + CC + GG + TT + AT + AG + AC + TA + TC + TG + GA + GC + GT + CA +CT + CG ,data=datos_completos_escalados_cor_sinpolyA, plot = "ROC", AUC=TRUE, MI = F)
```

En el caso del modelo resultante se puede apreciar que el área bajo la curva es de 0.927, y esto indica que **el modelo es muy bueno para clasificar**. Por otro lado, la gráfica resultante también indica **el punto de corte, que es de 0.897**.

## Estudio de normalidad de los datos

El **test de normalidad** se utiliza para comparar la función de distribución acumulada empírica de los datos de la muestra con la distribución esperada si los datos fueran normales [@chan2003biostatistics].

Para estudiar si los datos provienen de una población con distribución normal se disponen de tres herramientas principalmente:

 * Pruebas de hipótesis.
 * Histograma y/o densidad.
 * Gráficos cuantil cuantil (QQplot).

### Test de Lillefors

El **test de Kolmogorov-Smirnov** permite estudiar si una muestra procede de una población con una determinada distribución (media y desviación típica), no está limitado únicamente a la distribución normal. El Kolmogorov-Smirnov asume que se conoce la media y varianza poblacional, lo que en la mayoría de los casos no es posible. 

Para solventar este problema, fue desarrollada una modificación del Kolmogorov-Smirnov conocida como **test Lilliefors**. El test Lilliefors asume que la media y varianza son desconocidas, estando especialmente desarrollado para contrastar la normalidad. Es la alternativa al test de Shapiro-Wilk cuando el número de observaciones es mayor de 50.

A continuación se muestra dicho test con todas las variables para comprobar si siguen una distribución normal.

```{r}
# Test de Lillefors de todas las variables
apply(datos_filtradosyescalados_cor_completos[,2:29], MARGIN = 2, lillie.test)
```

La hipótesis nula de este Test de Lilliefors implica que los datos provienen de una población con distribución normal, mientras que la hipótesis alternativa sostiene lo contrario. Al observar los resultados de dicho test expuestos con anterioridad se puede rechazar esta hipótesis nula para todas las variables, y determinar que **no existe normalidad para ninguna de las variables**.

El hecho de no poder asumir la normalidad influye principalmente en los test de hipótesis paramétricos (t-test, anova,…) y en los modelos de regresión, que se han realizado con anterioridad.

Muestros test estadísticos requieren de normalidad en los datos, por lo que al saber que en este caso no se da dicha condición hay que tener precaución.

### Histograma y curva normal

A continuación se va representar los datos de algunas variables correlacionadas según los resultados de las distintas regresiones mediante un histograma. Además se va a superponer la curva de una distribución normal con la misma media y desviación estándar que muestran los datos.

Las variables que se van a visualizar son:

```{r, warning=FALSE}
# Histograma de normalidad de 'meanPd', 'meanEE' y mean_phastCons7way

h_nor_meanPd <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = meanPd)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$meanPd),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$meanPd))) +
  ggtitle("meanPd") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

h_nor_meanEE <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = meanEE)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$meanEE),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$meanEE))) +
  ggtitle("meanEE") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

h_nor_mean_phastCons7way <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = mean_phastCons7way)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$mean_phastCons7way),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$mean_phastCons7way))) +
  ggtitle("mean_phastCons7way") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

grid.arrange(h_nor_meanPd, h_nor_meanEE, h_nor_mean_phastCons7way, ncol = 3, widths = c(1,1,1))
```

Los resultados obtenidos como se puede apreciar implican que la representación gráfica no arroja la existencia de normalidad en el caso de las variables 'meanPd', 'meanEE' y 'mean_phastCons7way'.

```{r}
# Histogramas de normalidad de 'aphilicity', 'bdnatwist' y 'bendability'

h_nor_aphilicity <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = aphilicity)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$aphilicity),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$aphilicity))) +
  ggtitle("aphilicity") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

h_nor_bdnatwist <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = bdnatwist)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$bdnatwist),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$bdnatwist))) +
  ggtitle("bdnatwist") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

h_nor_bendability <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = bendability)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$bendability),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$bendability))) +
  ggtitle("bendability") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank())

grid.arrange(h_nor_aphilicity, h_nor_bdnatwist, h_nor_bendability, ncol = 3, widths = c(1,1,1))
```

En cuanto a los resultados obtenidos para los histogramas con curva normal de 'aphilicity', 'bdnatwist' y 'bendability' es que todas estas variables siguen una distribución típica de **Cauchy**, que es una distribución de probabilidad continua, y por tanto, no se aprecia una distribución típica de normalidad.

```{r}
# Histogramas de normalidad de 'proteindeformation', 'proteindnatwist' y 'cpnpgislands'

h_nor_proteindeformation <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = proteindeformation)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$proteindeformation),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$proteindeformation))) +
  ggtitle("proteindeformation") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 


h_nor_proteindnatwist <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = proteindnatwist)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$proteindnatwist),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$proteindnatwist))) +
  ggtitle("proteindnatwist") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

h_nor_cpnpgislands <- ggplot(data = datos_completos_escalados_cor_sinpolyA, aes(x = cpnpgislands)) +
  geom_histogram(aes(y = ..density.., fill = ..count..), colour="lightcoral", fill="lightcoral") +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "black",
                args = list(mean = mean(datos_completos_escalados_cor_sinpolyA$cpnpgislands),
                            sd = sd(datos_completos_escalados_cor_sinpolyA$cpnpgislands))) +
  ggtitle("cpnpgislands") +
  labs(y="densidad") +
  theme(axis.title.x = element_text(vjust=-0.5, size=rel(1))) +
  theme(axis.title.y = element_text(vjust=-0.5, size=rel(1))) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank()) 

grid.arrange(h_nor_proteindeformation, h_nor_proteindnatwist, h_nor_cpnpgislands, ncol = 3, widths = c(1,1,1))
```

Al igual que en el caso anterior, los resultados obtenidos para 'proteindeformation', 'proteindnastwist' y 'cpnpgislands' es que las tres muestran una distribución típica de **Cauchy**, y por ende, no se representa una distribución de normalidad.

### Gráficos cuantil cuantil (QQplot)

Los **gráficos QQplot** permiten comparar los cuantiles de la distribución observada con los cuantiles teóricos de una distribución normal con la misma media y desviación estándar que los datos. **Cuanto más se aproximen los datos a una normal, más alineados están los puntos entorno a la recta**.

A continuación se va a representar algunas variables correlacionadas según los resultados de las distintas regresiones mediante QQplot:

```{r}
# QQplot de 'meanPd', 'meanEE' y 'mean_phastCons7way'
qqplot_meanPd <- qplot(sample = meanPd, data = datos_completos_escalados_cor_sinpolyA, color="meanPd") +
labs(title="meanPd",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_meanEE <- qplot(sample = meanEE, data = datos_completos_escalados_cor_sinpolyA, color="meanEE") +
labs(title="meanEE",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_mean_phastCons7way<- qplot(sample = mean_phastCons7way, data = datos_completos_escalados_cor_sinpolyA, color="mean_phastCons7way", show.legend=FALSE)+
labs(title="mean_phastCons7way",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

grid.arrange(qqplot_meanPd, qqplot_meanEE, qqplot_mean_phastCons7way, ncol = 3, widths = c(1,1,1))
```

En el caso de las variables estudiadas con anterioridad mediante el QQplot ('meanPd', 'meanEE' y 'mean_phastCons7way'), se puede decir que ninguna de ellas se ve reflejada con la línea que se supone debería seguir para representar cierta normalidad en los datos.

```{r}
# QQplot de normalidad de 'aphilicity', 'bdnatwist' y 'bendability'
qqplot_aphilicity <- qplot(sample = aphilicity, data = datos_completos_escalados_cor_sinpolyA, color="aphilicity") +
labs(title="aphilicity",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_bdnatwist <- qplot(sample = bdnatwist, data = datos_completos_escalados_cor_sinpolyA, color="bdnatwist") +
labs(title="bdnatwist",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_bendability <- qplot(sample = bendability, data = datos_completos_escalados_cor_sinpolyA, color="bendability") +
labs(title="bendability",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

grid.arrange(qqplot_aphilicity, qqplot_bdnatwist, qqplot_bendability, ncol = 3, widths = c(1,1,1))
```

En este caso, las variables estudiadas con anterioridad, que corresponden a 'aphilicity', 'bdnatwist' y 'bendability' se corresponden algo a las distintas líneas de sus representaciones, pero en los extremos de las líneas se pierde la correspondencia, por lo que los resultados no son concluyentes para la normalidad.

```{r}
# QQplot de normalidad de 'proteindeformation', 'proteindnatwist' y 'cpnpgislands'
qqplot_proteindeformation <- qplot(sample = proteindeformation, data = datos_completos_escalados_cor_sinpolyA, color="proteindeformation") +
labs(title="proteindeformation",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_proteindnatwist <- qplot(sample = proteindnatwist, data = datos_completos_escalados_cor_sinpolyA, color="proteindnatwist") +
labs(title="proteindnatwist",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

qqplot_cpnpgislands <- qplot(sample = cpnpgislands, data = datos_completos_escalados_cor_sinpolyA, color="cpnpgislands") +
labs(title="proteindeformation",
       y = "Cuantiles de muestra", x="Cuantiles teóricos") +
geom_abline() +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1)), panel.grid = element_blank(), legend.position="none") 

grid.arrange(qqplot_proteindeformation, qqplot_proteindnatwist, qqplot_cpnpgislands, ncol = 3, widths = c(1,1,1))
```

Al igual que en el caso anterior, para las variables 'proteindeformation', 'proteindnastwist' y 'cpnpgislands' se ve cierta correspondencia con las distintas líneas que deben seguir, pero se pierde la misma cuando se llega a los extremos de las líneas, por lo que de nuevo, los resultados no son concluyentes para la normalidad.

Además de observar si hay o no normalidad en los datos, se va a proceder a estudiar si en las variables filtradas hasta ahora hay **presencia de valores fuera de rango**, es decir, si hay *outliers* en las variables. 

Cabe destacar que el gráfico que se va a visualizar a continuación corresponde a las **variables filtradas, pero ya escaladas, eliminando varianza 0 y centradas, así como también sin variables identificadas con correlación alta**:

```{r}
melt_variables_filtradas <- melt.data.frame(datos_filtradosyescalados_cor_completos[,2:30], id = "class")

ggplot(data=melt_variables_filtradas, aes(x=variable, y=value, color=variable, fill=variable)) +
  geom_boxplot(alpha=0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ggtitle("Diagramas de cajas de variables filtradas") +
  theme_bw() +
  ylab("Valores") +
  xlab("Variables") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  #theme(plot.title=element_text(hjust=0.5, size=rel(1)))
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
```

Como se puede observar se cuenta con presencia de *outliers* en todas las variables presentes, destacando los presentes en la variable 'meanEE', que presenta muchos con valores negativos a diferencia del resto.

Se va a investigar un poco qué pasa con esta variable, comparándola con la variable 'meanPd':

```{r}
# Variable 'meanEE' por classe:
variable_meanEE = filter(melt_variables_filtradas, variable == "meanEE")

ggplot(data=variable_meanEE, aes(x=value, color=class, fill=class)) +
  geom_density(alpha=0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("") +
  xlab("Valores") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank()) +
  facet_wrap(~ class, ncol=2,scales = "free") 
```

Como se puede apreciar, esta variable cuenta con presencia de valores negativos en todas las clases/regiones, lo que explicaría la presencia de valores fuera de rango que toman valores tan negativos, destacando mayoritariamente la visualización de ncRNA, que es en la que se ve que alcanza los valores más negativos.

```{r}
# Variable 'meanPd' por classe:
variable_meanPd = filter(melt_variables_filtradas, variable == "meanPd")

ggplot(data=variable_meanPd, aes(x=value, color=class, fill=class)) +
  geom_density(alpha=0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("") +
  xlab("Valores") +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank()) +
  facet_wrap(~ class, ncol=2,scales = "free") 
```

Si se comparan los resultados por clase de la variable 'meanPd' con 'meanEE' se puede apreciar que esta presenta algunos valores fuera de rango que toman valores positivos, lo que queda justificado al ver los valores que toma dicha variable en las distintas clases, que se diferencia de forma clara con lo que se encontró previamente en 'meanEE', que tomaba valores negativos.


## Análisis no supervisado

### Análisis de componentes principales (PCA)
Como se cuenta con unos dataset que tiene un gran número de variables correlacionadas, se puede realizar un **análisis de componentes principales** (PCA). Es un método estadístico que permite simplificar la complejidad (reducción de la dimensionalidad) de espacios muestrales con muchas dimensiones a la vez que conserva su información [@abdi2010principal].

```{r}
# PCA
datos_pca = PCA(datos_completos_escalados_cor_sinpolyA, quali.sup = 29, ncp = 28, graph = F, 
    scale.unit = F)
```

Una vez realizado el análisis PCA y obtenidos los datos que se requieren, se puede proceder a la representación gráfica. Se van a representar las dos primeras componentes extraídas del PCA, diferenciando dos gráficas distintas: una para las clases, y otra para el resto de variables numéricas. La razón para escoger las dos primeras componentes es porque esas son las que mejor explican la varianza de los datos. El *primer componente principal* es un vector, con un valor para cada ejemplo del dataset, y se define como la combinación lineal de las variables originales que tiene varianza máxima, mientras que el *segundo componente principal* describe la variabilidad en los datos que no está correlacionada en con el primer componente principal.

```{r}
# Gráfico de las clases por PCA
PCA1_clases <- fviz_pca_ind(datos_pca, geom.ind = "point", col.ind = datos_completos_escalados_cor_sinpolyA$class, 
addEllipses = TRUE, ellipse.level=0.95, title = "Gráfico de clases - PCA", legend.title = "Tipo de clase") +
scale_color_brewer(palette = "Set2") +
theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
PCA1_clases

# Gráfico de las variables por PCA
PCA1_variables <- fviz_pca_var(datos_pca, col.var="steelblue", title="Gráfico de variables - PCA", legend.title = "Variables")+ theme_bw() +
theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank()) 
PCA1_variables
```

* Si se observa la representación resultante del **gráfico de clases** se puede apreciar que todas las clases se encuentran muy centradas, con la única excepción de 5'-UTR, que abarca una anchura superior al resto de clases. También 3'-UTR se desvía un poco de las otras, pero ICE, lncRNA, ncRNA y pseudogene se encuentran centradas y muy cercanas entre sí.

* Si se observa el **gráfico de las variables** se puede ver cómo, por ejemplo, variables como las medias de 'meanEE' y 'mean_phastCons7way' se encuentran muy cerca entre sí, mientras que 'meanPd' se encuentra totalmente enfrentada a ellas. Por otro lado se destaca que algunas propiedades termodinámicas y topológicas de las secuencias como 'aphilicity' y 'bendability' se encuentran totalmente enfrentadas, ya que 'aphilicity' formaría parte de la primera componente principal mientras que 'bendability' no estaría bien representada para ninguna de las dos componentes principales.

A continuación, tras este estudio preliminar, se va a proceder a finalizar este análisis de componentes principales mostrando los **10 predictores que más información aportan en cada una de las 2 componentes principales del conjunto de datos**, utilizando como indicador de su importancia la varianza total que son capaces de explicar.

```{r}
# Vectores más significativos de la primera dimensión
var1 <- head(sort(datos_pca$var$cos2[, 1], decreasing = T), 
    10)
# Vectores más significativos de la segunda dimensión
var2 <- head(sort(datos_pca$var$cos2[, 2], decreasing = T), 
    10)

knitr::kable(var1, caption = "Vectores más significativos de la primera dimensión en el conjunto de datos") %>%
  kable_styling(bootstrap_options = "condensed")

knitr::kable(var2, caption = "Vectores más significativos de la segunda dimensión en el conjunto de datos") %>%
  kable_styling(bootstrap_options = "condensed")
```


Como se puede apreciar y se ha mencionado con anterioridad, la variable **'aphilicity' es el predictor que más información arroja en la primera componente principal**, seguido de 'proteindeformation' y de 'GC', aunque este último ya con una bajada considerable de la varianza (0.6862897).

Si se observan ahora los resultados de la segunda componente principal, se puede observar que **el predictor que más información arroja a la segunda componente principal es 'GA'**, con una varianza de 0.6224029, tal y como se puede visualizar en el gráfico anterior (*Gráfico de las variables por PCA*).

### UMAP
El UMAP es una de las técnicas más usada el la actualidad junto con t-SNE para la reducción de la dimensionalidad para visualización. En algunos casos es más efectiva que t-SNE, más eficiente computacionalmente y mejor escalable para un número grande de dimensiones.

Para poder aplicar este tipo de reducción de dimensionalidad hay que utilizar la función `umap()`, del paquete del mismo nombre. 

A continuación se mostrarán los resultados obtenidos tras realizar el `umap()`en los datos que se utilizan en el estudio.

```{r}
# Los datos son convertidos en una matriz
datos_matriz <- as.matrix(datos_filtradosyescalados_cor_completos[2:29])
```

```{r, warning=FALSE, eval=FALSE}
# Se realiza el umap
umap_resultados <- umap(datos_matriz)
# Summary del umap
summary(umap_resultados)
```
```{r}
umap_resultados <- readRDS("./umap_resultados.RDS")
```

Una vez obtenidos los resultados se va a proceder a la representación gráfica del umap:

```{r, warning=FALSE}
# Representación gráfica de umap
colnames(umap_resultados$layout) = c("Dim1","Dim2")
umapaxes_a = as.data.frame(umap_resultados$layout)%>% mutate(class=datos_filtradosyescalados_cor_completos$class)
umapaxes_b = cbind(State = datos_completos_escalados_cor_sinpolyA$class,umapaxes_a)
ggplot(umapaxes_b, aes(Dim1, Dim2, color=State,fill=State, alpha=0.6)) + 
  modelr::geom_ref_line(h = 0) +
  modelr::geom_ref_line(v = 0) +
  scale_y_continuous(name = "Dim2", limits = c(-9,10))+
  scale_x_continuous(name = "Dim1", limits = c(-9,10))+
  geom_point() + scale_shape(solid=F) +
  scale_color_brewer(palette = "Set2") +
  ggtitle("UMAP") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(plot.title=element_text(hjust=0.5, size=rel(1.5)), panel.grid = element_blank())
```

UMAP intenta encontrar una representación (no lineal) de pocas dimensiones de los datos que preserve las distancias entre cada puntos y sus vecinos en el espacio multi-dimensional. Aunque UMAP tiende a encontrar clusters visualmente más compactos, si se aprecia la representación anterior se puede obtener poca información, ya que las clases se encuentran muy centradas en el gráfico y solapadas entre sí, por lo que no se distingue diferenciación alguna.

## División del dataset

Para encarar la segunda parte del estudio, que consta de la realización de los modelos de predicción de machine learning que van a hacer posible distinguir entre regiones 3-UTR de las que no lo son, es necesario hacer una selección de variables. Antes de esto, **se va a proceder a dividir el dataset principal según dos características: 3-UTR y NO 3-UTR**.

Se toma esta decisión debido a que el objetivo es la predicción de regiones 3-UTR de las que no lo son, por lo que es indiferente que dentro de las regiones que no son UTR el modelo distinga entre otras clases. Se creará este subconjunto con el objetivo de tener un mayor ahorro computacional y para poder responder de una forma más clara la incógnita principal.

```{r}
#Creación de subconjunto de datos en dos clases: 3UTR y no 3UTR
datos_3UTR <- datos_filtradosyescalados_cor_completos
datos_3UTR$class<-factor(datos_3UTR$class, levels = c( "UTR","5-UTR", "ICE", "lncRNA", "ncRNA", "pseudoGene"), labels=c("3-UTR","No-3UTR", "No-3UTR", "No-3UTR", "No-3UTR", "No-3UTR"))
```

```{r}
datos_3UTR <- readRDS("./datos_3UTR.RDS")
```

De esta forma se obtiene un subconjunto de datos que distingue **dos únicas clases de entre los datos: 3-UTR y No-3UTR**.

## Selección de variables

Este apartado corresponde a la selección de variables a estudiar en el futuro de este análisis, con el objetivo de eliminar aquellas que no aporten ningún valor al conjunto de datos.

Previamente se ha procedido a la eliminación de variables con **varianza cercana a 0**, así como a la eliminación de las **variables altamente correlacionadas**. Esto también evita introducir información redundante al modelo y puede mejorar la precisión del mismo. También se ha trabajado hasta el momento con datos centrados y escalados.

Se va a realizar primero una **selección recursiva de variables** y tras esto, una **selección por filtro de variables**. La selección recursiva de variables se usa debido a que es fácil de configurar y usar y porque es eficaz, mientras que la selección por filtro de variables se usa para filtrar las columnas redundantes del modelo y para comparar los resultados que se obtienen de dicho método con el de selección recursiva de variables.

### Selección recursiva de variables

En primer lugar se va a proceder a eliminar aquellas variables que no tengan ningún valor a la hora de predecir el tipo de región a la que pertenecen. Para este cometido se va a emplear el paquete `Caret` y la función `rfe()`. Esta herramienta va a seleccionar de forma recursiva las variables que de verdad son útiles para predecir. 

Se van a seleccionar los hiperparámetros y parámetros que van a determinar el funcionamiento del análisis. En primer lugar hay que especificar el algoritmo que se va a usar, siendo en este caso `treebagFuncs`, ya que permite mejorar la precisión del modelo. El método que se va a utilizar para evitar el *overfitting* y obtener el mejor modelo en este caso es *crossvalidation*.

Además de esos parámetros, hay que seleccionar las distintas semillas a utilizar y el número de *folds* que van a ser necesario a la hora de realizar el modelo. En este caso se han realizado 10 folds. También es necesario elegir el rango del **número de variables que van a ser elegidas en cada caso**. Se ha decidido ir de un mínimo de 3 al total de variables.

Este paso del análisis se va a realizar de forma paralela utilizando 5 núcleos en cada tarea, reduciendo de forma significativa el tiempo que tarda en procesar los datos.

Al final de este proceso se obtendrán las variables elegidas por que aportan información al modelo y se podrán eliminar del conjunto de datos.


```{r, eval=FALSE}
# Paralelización del proceso
cl <- makeCluster(5)
registerDoParallel(cl)

# Parámetros de selección recursiva de variables
subsets <- c(3:(ncol(datos_3UTR) - 1))
seeds <- vector(mode = "list", length = 6)
for(i in 1:10) seeds[[i]] <- sample.int(1000, length(subsets) + 1)
seeds[[11]] <- sample.int(1000, 1)

control_seleccion <- rfeControl(functions=treebagFuncs, 
                                     method = "cv", 
                                     number = 10, 
                                     seeds = seeds,
                                     returnResamp="final", 
                                     verbose = TRUE,
                                     allowParallel = TRUE)

# Selección de variables recursivas
if (file.exists("modelo_seleccion_recursiva")) {
    modelo_seleccion_recursiva <- readRDS("modelo_seleccion_recursiva")
} else {
    modelo_seleccion_recursiva <- rfe(class~., data=datos_3UTR, 
        sizes = subsets, rfeControl = control_seleccion)
    saveRDS(modelo_seleccion_recursiva, "modelo_seleccion_recursiva")
}

modelo_seleccion_recursiva$fit = NULL
```

```{r}
modelo_seleccion_recursiva <- readRDS("./modelo_seleccion_recursiva.RDS")
```

```{r}
# Variables significativas tras selección recursiva de variables
variables_significativas_recursiva <- modelo_seleccion_recursiva$optVariables

# Variables que mejor clasifican el conjunto de entrenamiento
variables_significativas_recursiva
```

La variable `modelo_seleccion_recursiva` indica que el **mejor clasificador que clasifica los objetos del conjunto de entrenamiento contiene 24 variables**. Dichas variables quedan almacenadas en el atributo `optVariables`.

Los resultados del proceso de selección de variables también pueden ser mostrados por medio de gráficas:

```{r}
# Representación de resultados tras selección recursiva de variables
plot(modelo_seleccion_recursiva, col="lightcoral", main="Selección recursiva de variables")
```

En la gráfica se enfrentan las variables seleccionadas y el punto de corte con el accuracy. De las 29, 24 variables sería lo más óptimo según el algoritmo de clasificación.

### Selección de variables por filtros

Además de la selección recursiva, se va a utilizar el **método de selección por filtro** para poder tener alguna referencia extra a la hora de seleccionar las variables. Para ello se hará uso de la función `sbf` de `caret`, siguiendo un procedimiento similar al anterior. Esta función implementa un test estadístico, que trata de encontrar aquellas variables que presentan diferencias significativas estadísticamente entre las distintas clases o el resultado.

El procedimiento que se sigue es similar al del caso anterior:

```{r, eval=FALSE}
# Paralelización del proceso
cl <- makeCluster(5)
registerDoParallel(cl)

# Parámetros de selección de variables por filtros
ctrl.ranker.cv.10 <- sbfControl(functions = treebagSBF,
                                method = "cv", number = 10,
                                seeds = NULL)

# Selección de variables por filtros
if (file.exists("modelo_seleccion_filtro")) {
    modelo_seleccion_filtro <- readRDS("modelo_seleccion_filtro")
} else {
    modelo_seleccion_filtro <- sbf(class~., data=datos_3UTR, 
                            sbfControl = ctrl.ranker.cv.10)
    saveRDS(modelo_seleccion_filtro, "modelo_seleccion_filtro")
}

modelo_seleccion_filtro$fit = NULL
```

```{r}
modelo_seleccion_filtro <- readRDS("./modelo_seleccion_filtro.RDS")
```

```{r}
# Variables significativas tras selección de variables por filtros
variables_significativas_filtro <- modelo_seleccion_filtro$optVariables

# Variables que mejor clasifican el conjunto de entrenamiento
variables_significativas_filtro
```

En el caso de la selección de variables por filtro, como se puede observar, se seleccionan todas las variables disponibles.


### Comparación de selección de variables
Una vez se generan los modelos de selección de variables se puede proceder a su comparación estadística, y mediante un diagrama de Venn, comparar qué variables son las que varían dependiendo del modelo (recursivo o por filtro).

```{r}
# Estadísticos de selección recursiva de variables
kable(modelo_seleccion_recursiva$results[modelo_seleccion_recursiva$results == modelo_seleccion_recursiva$optsize, 
    ], caption = "Estadísticos de selección recursiva de variables") %>%
  kable_styling(bootstrap_options = "condensed")
```
```{r}
# Estadísticos de selección por filtro de variables
kable(modelo_seleccion_filtro$results, caption="Estadísticos de selección por filtro de variables") %>%
  kable_styling(bootstrap_options = "condensed")
```

Como se puede observar, la *Accuracy* (porcentaje de casos que el modelo ha acertado) es ligeramente mejor en el modelo de selección recursiva de variables que en el de selección de variables por filtro. La diferencia es muy poco notoria, por lo que se puede concluir que ambos modelos son muy buenos.

Si se observan los valores de *Kappa* (mide la fiabilidad de los calificadores) tampoco se observa una diferencia notoria, aunque los valores de Kappa en el caso de la selección recursiva de variables es mejor que en el caso de la selección de variables por filtro, comportándose de forma similar a lo que sucede en el caso de los valores de *Accuracy*.

Como conclusión, destacar que la selección recursiva de variables presente mejores valores de accuracy y kappa que la selección por filtro de variables y que además esta primera no selecciona cinco variables frente a la selección de variables por filtro que selecciona todas las variables. Se considera entonces que esta **selección recursiva de variables es mejor que la de filtro y las variables que este método tiene en consideración serán las seleccionadas para futuros análisis**.

```{r}
# Comparación de selección de variables entre selección recursiva y por filtro
comparacion_significativas_2modelos = list(recursiva = variables_significativas_recursiva, filtro = variables_significativas_filtro)

venn(comparacion_significativas_2modelos, ilab = TRUE, zcolor = "style", 
    box = F, cex.main=0.5)
title("\nComparación de métodos de selección de variables")
```

Como se puede apreciar en la representación anterior **hay una clara diferencias entre el método de selección recursiva de variables y el método de selección por filtro de variables, ya que el método de selección recursiva de variables no selecciona cinco variables que el método de selección por filtro de variables sí**, por lo que se puede concluir que la selección por filtro de variables arroja un mejor resultado y será la que se tenga en consideración para llevar a cabo la segunda parte del estudio, en la que se llevará a cabo la realización de los modelos.

### Análisis de resultados de selección de variables

Tras tomar la decisión de adoptar los resultados obtenidos de la selección recursiva de variables para la realización de los modelos posteriormente, se pueden observar las variables escogidas por dicho método e intentar encontrar un **sentido biológico a la relación de las mismas con las regiones 3'-UTR**.

Como se vio en el análisis de correlación, en el modelo 3'-UTR, la única variable de todas que NO estaba correlacionada era 'cpnpgislands'. Estas regiones son regiones de DNA donde un nucleótido de citosina es seguido por un nucleótido de guanina en la secuencia lineal de bases a lo largo de su dirección 5' → 3'. En humanos, alrededor del 70% de los promotores ubicados cerca del sitio de inicio de la transcripción de un gen contienen una isla CpG [@saxonov2006genome].
Tiene sentido entonces que esta variable resulte de utilidad para detectar la no presencia de 3'-UTR a priori, ya que su ubicación en el genoma suele ser la opuesta a donde se encontraría un 3'-UTR. 
Estas islas CpG se caracterizan por mostrar un nucleótido de citosina seguido de otro de guanina a lo largo de su dirección 5' → 3'. Es por esto que otros predictores importantes señalados son 'CG', 'C' y 'G'.

Por otro lado también cabe destacar la implicación de la cola PolyA en la identificación de las regiones 3'-UTR. Esto es debido a que seguida de estas regiones 3'-UTR se puede encontrar la presencia de esta otra región no traducida, la cola PolyA, como se puede ver en la Figura expuesta a continuación. Es por esto por lo que resulta un predictor muy importante a la hora de identificar estas regiones 3'-UTR.

![mRNA de una proteína típica humana incluyendo las regiones no traducidas](figura2.png) 

Otro predictor importante es 'proteindeformation', ya que las regiones 3'-UTR está implicadas en la correcta formación de proteínas [@mayr2017regulation], al igual que pasa con las variables 'bendability' y 'aphilicity', que son propiedades topológicas y termodinámicas que se asocian a las proteínas y que también están muy relacionadas con estas regiones 3'-UTR por este mismo motivo: la correcta formación de las proteínas.

SEGUNDA PARTE

## Obtención y preparación de los datos
Para realizar la segunda parte del estudio va a ser necesario cargar los datos generados previamente en la entrega 1 del estudio que hacen referencia al dataset inicial dividido en dos únicas clases: 3'-UTR y NO 3'-UTR. 

Estos son los datos que interesa introducir en los modelos que se realizarán posteriormente, con el objetivo de poder predeccir la presencia de este tipo de regiones en el genoma humano.

```{r}
# Datos necesarios
datos_3UTR <- readRDS("./datos_3UTR.RDS")
datos_3UTR$class<-factor(datos_3UTR$class, levels = c("3-UTR","No-3UTR"), labels=c("Si3UTR","No3UTR"))
```

### División del dataset en train y test

Una vez cargados los datos hay que tener en cuenta la necesidad de seleccionar aquellas variables que el método de selección recursiva de variables indicó que eran las óptimas, así como dividir los datos en un **subconjuntos de entrenamiento y subconjuntos de test** con el fin de poder pasar posteriormente a la realización de los modelos.

* Los **datos de entrenamiento** o *training data* son los datos que se usan para entrenar el modelo, y la calidad de los futuros modelos de aprendizaje automático va a ser directamente proporcionales a la calidad de estos datos.

* Por otro lado, los **datos de prueba**, validación o *testing data* son los datos que se reservan para comprobar si el modelo que se ha generado a partir de los datos de entrenamiento funciona. Es decir, si las respuestas predichas por el modelo para un caso son acertadas o no.

Normalmente el conjunto de datos se suele repartir en un 70-80% de datos de entrenamiento y un 30-20% de datos de test. Lo importante es ser siempre conscientes de que hay que evitar el sobreajuste u *overfitting*. El *overfitting* es el efecto de sobreentrenar un algoritmo de aprendizaje con unos ciertos datos para los que se conoce el resultado deseado.

En este caso y para este estudio particular se decidió dividir los datos en un 80% de entrenamiento y un 20% de test. Para ello se utilizó la función `createDataPartition` de `Caret`, que permite mantener la misma proporción de los datos en los dos subconjuntos. 

```{r, eval=FALSE}
# Lectura del modelo de selección recursiva de variables
modelo_seleccion_recursiva <- readRDS("./modelo_seleccion_recursiva.RDS")
```


```{r, warning=FALSE, eval=FALSE}
set.seed(123)
# Se seleccionan las variables seleccionadas por el método de selección recursiva
significativas_modelo_recursivo <- modelo_seleccion_recursiva$optVariables
significativas_modelo_recursivo <- c("polyA_signal", significativas_modelo_recursivo[-3])

datos_seleccion_de_variables = dplyr::select(datos_3UTR, all_of(significativas_modelo_recursivo), class)

# Se dividen los datos en train y test
trainIndex <- createDataPartition(datos_seleccion_de_variables$class, p=0.8,list = FALSE, times = 1)
datos3UTR_train <- datos_seleccion_de_variables[trainIndex,]
datos3UTR_test <- datos_seleccion_de_variables[-trainIndex,]
```

```{r}
datos3UTR_train <- readRDS("./datos3UTR_train.RDS")
datos3UTR_test <- readRDS("./datos3UTR_test.RDS")
```

### Desbalanceo de los datos

Con anterioridad, en la entrega 1, se habló del claro desbalanceo que muestran los datos que se presentan en este estudio. Un conjunto de **datos desbalanceado es uno donde el número de observaciones pertenecientes a un grupo o clase es significativamente mayor que las pertenecientes a las otras clases**. 

En el dataset usado en el estudio se puede ver una clara diferencia entre las dos clases presentes, ya que **predomina la No presencia de 3-UTR (clase No3UTR) frente a la presencia de 3-UTR (clase SiUTR)**:

```{r}
# Datos sin balancear
datos_sin_balancear <- table(datos_3UTR$class)
knitr::kable(datos_sin_balancear, caption = "Tipo de clase",col.names = c("Clases", "Frecuencia")) %>% kable_styling(bootstrap_options = "condensed")
```


Este problema se debe solucionar, ya que este fenómeno provoca un desbalanceo en los datos que se utilizan para el entrenamiento de los modelos, y por lo general, afecta a los algoritmos en su proceso de generalización de la información, perjudicando a las clases minoritarias.

En esta ocasión se ha decidido utilizar las funciones `downSample` y `upSample`, que muestrearán aleatoriamente de forma totalmente inversa: en el caso `downSample`, muestreará el conjunto de datos para que todas las clases tengan la **misma frecuencia que la clase minoritaria** (disminuye la clase mayoritaria a favor de la minoritaria), y así solucionar este problema de desbalanceo de los datos, mientras `upSample` muestrerá para hacer que la clase minoritaria alcance la misma frecuencia que la clase mayoritaria. Al aplicar ambos métodos se verá que el resultado de `downSample` reduce el número de muestras mientras que `upSample`, por el contrario, las aumentará.

Cabe destacar que este proceso se llevará a cabo **con los datos con los que se va a entrenar los modelos**, ya que son los datos que se necesita que estén balanceados por lo expuesto con anterioridad.

```{r, eval=FALSE}
# Balanceo de los datos 
datos3UTRtrain_down <- downSample(datos3UTR_train[, -ncol(datos3UTR_train)], datos3UTR_train$class)
datos3UTRtrain_up <- upSample(datos3UTR_train[, -ncol(datos3UTR_train)], datos3UTR_train$class)
```

```{r}
datos3UTRtrain_down <- readRDS("./datos3UTRtrain_down.RDS")
datos3UTRtrain_up <- readRDS("./datos3UTRtrain_up.RDS")
```


```{r}
# Tabla de los datos balanceados

  # downSample
down <- table(datos3UTRtrain_down$Class)
knitr::kable(down, caption = "Número de clases con downSample",col.names = c("Clases", "Frecuencia")) %>% kable_styling(bootstrap_options = "condensed")

  # upSample
up <- table(datos3UTRtrain_up$Class)
knitr::kable(up, caption = "Número de clases con upSample",col.names = c("Clases", "Frecuencia")) %>% kable_styling(bootstrap_options = "condensed")

```

Como se dijo anteriormente, si se observan los datos resultantes tras aplicar `downSample`, se ve cómo se reduce significativamente el tamaño de los datos, ya que se pasa de 179968 observaciones a 28352 en total, lo que afectará positivamente al tiempo de cómputo de los modelos que se realizarán posteriormente. Por otro lado, como se observa, en el caso de aplicar `upSample`, se pasa de 179968 observaciones a 259600 observaciones, aumentando significativamente el número de muestras. Esto afectará negativamente al tiempo de cómputo en los modelos que se implementarán posteriormente.

A continuación se muestran dos barplot con las proporciones relativas de ambos balanceos. Se podrá comprobar que pese a afectar al número de muestras de forma distinta, se consigue el balanceo de los datos en ambos casos de forma satisfactoria:

```{r}
# Frecuencia relativa de los datos balanceados
balanceo_relativo_down <- prop.table(down)
balanceo_relativo_up <- prop.table(up)

# Representación de la distribución de clases en frecuencia relativa
barplot(balanceo_relativo_down, col = palette("Pastel 1"), ylab ="Frecuencias Relativa", ylim = c(0, 1), main = "Distribución de clases - downSample")

barplot(balanceo_relativo_up, col = palette("Pastel 1"), ylab ="Frecuencias Relativa", ylim = c(0, 1), main = "Distribución de clases - upSample")
```

Como se puede comprobar los histogramas anteriores, **se ha producido el balanceo de los datos de forma satisfactoria en ambos casos**, ya que en este momento se tiene la **misma frecuencia de datos de clase 3-UTR que de clase No-3UTR**. 

# Desarrollo - Segunda entrega

Esta parte del estudio se va a centrar en la **generación de modelos de machine learning, su evaluación y la selección del mejor modelo final**. Se debe responder a las siguientes cuestiones:

* Determinar cuál es el mejor algoritmo de predicción y por qué.
* Para cada algoritmo que se use, determinar la estrategia de optimización de hiperparámetros usada, el significado de cada hiperparámetro considerado, su configuración de caret por defecto y cómo se ha decidido configurarlos.
* Qué medidas de rendimiento se han elegido para reportar y decidir el mejor modelo y qué estrategia de remuestreo se ha seguido para obtener buenos estimadores de las medidas de rendimiento.
* Cuál es el mejor algoritmo para este problema y por qué.
* Una vez seleccionado el mejor modelo, determinar cuáles son las *features* o predictores más relevantes para el modelo seleccionado. Si el mejor modelo no permite determinar la importancia de las variables, se debe escoger el mejor modelo que lo permita y responder a esta pregunta.

## Realización de los modelos

A partir de los predictores seleccionadas en la anterior parte del estudio, y con los datos divididos en train y test como se procedió anteriormente, se elaborarán modelos basados en **random forests** y modelos de **deep learning**, así como otros modelos que resulten de interés, con el objetivo de poder responder a través de ellos a la pregunta inicial de si es posible distinguir regiones 3'-UTR en el genoma humano.

Los modelos de Random forest y de Deep learning se generarán con los datos balanceados mediante `downSample` y con los datos balanceados con `upSample` con el objetivo de comparar el rendimiento de los modelos dependiendo del tipo de balanceo que se le dio a los datos. El resto de modelos que se presentarán, sólo se realizaron con los datos balanceados mediante downSample, ya que este conjunto de datos posee menos de la mitad de muestras que el balanceado con upSample y eso facilitará el tiempo de cómputo y generación de los modelos. 

Se expondrán las diferencias entre el Random forest y el Deep learning dependiendo del conjunto de datos, así como también las diferencias entre los dos mejores modelos, pero en este caso, solo con el conjunto de datos balanceado mediante `downSample`.

### Random forest

**Random forest** es un algoritmo de aprendizaje automático supervisado muy usado en problemas de clasificación y regresión . Este algoritmo construye árboles de decisión en diferentes muestras y toma su voto mayoritario para la clasificación y el promedio en caso de regresión [@azar2014random].

Una de las características más importantes del Random Forest es que puede manejar un conjunto de datos que contenga variables continuas como en el caso de la regresión y variables categóricas como en el caso de la clasificación. **Ofrece mejores resultados para problemas de clasificación, como es el caso de este estudio**, por lo que será un buen algoritmo para conseguir el objetivo planteado.

Para generar el modelo final de Random forest, se utilizará el algoritmo `ranger`, una implementación rápida de los Random forest, adecuada para datos dimensionales y para problemas de clasificación. Es un tipo de función que permite la modificación de los parámetros `mtry`, `splitrule` y `min.node.size`.

Se buscará entonces información de los valores por defecto de los parámetros de dicho algoritmo:

```{r}
# Información sobre algoritmo `ranger`
getModelInfo("ranger")$ranger$grid
```
Como se ve, en los valores por defecto de `ranger`, el tamaño mínimo de los nodos es 1 y el valor de splitrule "gini" si se quiere predecir sobre variables categóricas, como es el caso de este estudio. 

Se definirán ahora los parámetros:

* `mtry`: número predictores seleccionados aleatoriamente en cada árbol.
* `splitrule`: criterio de división. Para la clasificación y estimación de probabilidad usa "gini", "extratrees" o "hellinger", con "gini" por defecto. Para la regresión, usa "varianza", "extratrees", "maxstat" o "beta", con "varianza" por defecto. Para la supervivencia, usa "logrank", "extratrees", "C" o "maxstat" con "logrank" por defecto.
* `min.node.size`: Tamaño mínimo del nodo. Por defecto usa 1 para clasificación, 5 para regresión, 3 para supervivencia
y 10 para probabilidad.

```{r}
# Parámetros que requiere un ranger forest
ranger <- table(modelLookup("ranger"))
knitr::kable(ranger, caption="Hiperparámetros que requiere un Ranger forest") %>% kable_styling(bootstrap_options = "condensed")
```
Aunque se obtuvo información relativa a los parámetros necesarios para implementar en este algoritmo, se decidió utilizar la función `tuneLength`. Esta función permite la automatización del proceso para encontrar la mejor combinación de parámetros. Lo que hace es indicar al algoritmo que pruebe diferentes valores por defecto para el parámetro principal y encuentra el mejor. Se ha seleccionado que esta función pruebe un máximo de 10 combinaciones de estos hiperparámetros, aunque esto depende del número de predictores y tamaño de los datos.

Mediante *trainControl* se selecciona el método para evitar el sobreajuste. En este caso se usó *cross-validation* y con 10 folders. *Cross-validation* es una técnica utilizada para evaluar los resultados de un análisis estadístico y que garantiza que son independientes de la partición entre datos de entrenamiento y prueba [@a2019evaluation]. Esta técnica ya se usó anteriormente en la primera parte del estudio.

Por último, destacar que se paralelizó el proceso usando 5 núcleos.

```{r}
# Ranger forest de datos de entrenamiento balanceados con downSample
cl <- makeCluster(5)
registerDoParallel(cl)

set.seed(123)
random_forest_control <- trainControl(method = "cv", number = 10, 
                                      returnResamp = "final", verboseIter = T, allowParallel = T)

set.seed(123)
if (file.exists("rangerforest_downSample")) {
  rangerforest_downSample <- readRDS("rangerforest_downSample")
} else {
  rangerforest_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, method = "ranger", 
                                       tuneLength = 10, trControl = random_forest_control)
  saveRDS(rangerforest_downSample, "rangerforest_downSample")
}
```

A continuación, se van a exponer los resultados obtenidos, los mejores ajustes que ha encontrado el algoritmo para los hiperparámetros y la representación gráfica de estos resultados: 

```{r}
# Resultados obtenidos 
print(rangerforest_downSample)
```

```{r}
# Mejores ajustes de hiperparámetros
knitr::kable(rangerforest_downSample$bestTune, caption="Mejores ajustes de hiperparámetros") %>% kable_styling(bootstrap_options = "condensed")
```

```{r}
# Representación gráfica de Ranger forest con datos balanceados con downSample
ggplot(rangerforest_downSample, highlight = TRUE) +
  scale_x_continuous(breaks = 1:30) +
  labs(title = "Evolución del accuracy del modelo Random Forest") +
  guides(color = guide_legend(title = "mtry"),
         shape = guide_legend(title = "mtry")) +
  theme_classic()
```


Estos resultados indican que usando *Ranger forest* con `mtry`=6, `min.node.size`= 1 y `splitrule` = "gini", se consigue un *accuracy* promedio de validación del 94% en el caso de los datos balanceados con `downSample`, siendo este un muy buen resultado.

A continuación se va a proceder a la implementación del algoritmo con los datos balanceados mediante `upSample` de la misma forma:

```{r}
# Ranger forest de datos de entrenamiento balanceados con upSample 
cl <- makeCluster(5)
registerDoParallel(cl)

set.seed(123)

random_forest_control <- trainControl(method = "cv", number = 10, 
    returnResamp = "final", verboseIter = T, allowParallel = T)

set.seed(123)
if (file.exists("rangerforest_upSample")) {
    rangerforest_upSample <- readRDS("rangerforest_upSample")
} else {
    rangerforest_upSample <- caret::train(Class ~ ., data = datos3UTRtrain_up, method = "ranger", 
        tuneLength = 10, trControl = random_forest_control)
    saveRDS(rangerforest_upSample, "rangerforest_upSample")
}
```

A continuación, se van a exponer los resultados obtenidos, los mejores ajustes que ha encontrado el algoritmo para los hiperparámetros y la representación gráfica de estos resultados: 

```{r}
# Resultados obtenidos 
print(rangerforest_upSample)
```

```{r}
# Mejores ajustes de hiperparámetros
knitr::kable(rangerforest_upSample$bestTune, caption="Mejores ajustes de hiperparámetros") %>% kable_styling(bootstrap_options = "condensed")
```

```{r}
# Representación gráfica de Ranger forest con datos balanceados con downSample
ggplot(rangerforest_upSample, highlight = TRUE) +
  scale_x_continuous(breaks = 1:30) +
  labs(title = "Evolución del accuracy del modelo Random Forest") +
  guides(color = guide_legend(title = "mtry"),
         shape = guide_legend(title = "mtry")) +
  theme_classic()
```

Estos resultados indican que usando *Ranger forest* con `mtry=2`, `min.node.size= 1` y `splitrule = "gini"`, se consigue un *accuracy* promedio de validación del 99% en el caso de los datos balanceados con `upSample`, siendo este un resultado casi perfecto.

### Deep learning 

El **Deep learning** es un tipo de *machine learning* que entrena a una computadora para que realice tareas como las propias del ser humano, como por ejemplo, el reconocimiento del habla, la identificación de imágenes o hacer predicciones. En lugar de organizar datos para que se ejecuten a través de ecuaciones predefinidas, el Deep learning configura parámetros básicos acerca de los datos y entrena a la computadora para que aprenda por cuenta propia reconociendo patrones mediante el uso de muchas capas de procesamiento [@min2017deep].

El paquete `Caret` dispone de distintas funciones que permiten realizar una optimización de reeds neuronales. En este caso en particular se va a hacer uso del algoritmo `mlpKerasDropout` por simplicidad. 

A continuación se procede a presentar información sobre este algoritmo, el cómo se implementa y sus valores por defecto:

```{r}
# Información sobre algoritmo `mlpKerasDropout`
getModelInfo("mlpKerasDropout")$mlpKerasDropout$grid
```

En la ejecución anterior se pueden apreciar los valores por defecto que toma el algoritmo `mlpKerasDropout` en todos sus hiperparámetros: `size = ((1:len) * 2) - 1, dropout = seq(0, 0.7, length = len), batch_size = floor(nrow(x)/3), lr = 2e-06, rho = 0.9, decay = 0, activation = "relu"`.

En este caso particular de implementación de este algoritmo se va a hacer uso de `tuneGrid` para la definición de los distintos hiperparámetros que se deben escoger para generar el mejor modelo posible. En este caso, antes de definir el modelo definitivo, se realizaron diferentes pruebas modificando cada uno de los hiperparámetros haciendo un proceso de optimización de la red, partiendo de los valores que toma el algoritmo como preedefinidos para este caso, llegando a modificar cada uno de ellos para ver cómo iba mejorando los valores de accuracy del modelo.

Los hiperparámetros de este algoritmo son: `size`, `dropout`, `batch_size`, `lr`, `rho`, `decay`, y `activation`. Todos ellos son parámetros numéricos, excepto el último (`activation`), que es *character*.

```{r}
# Hiperparámetros para el algoritmo MLP con dropout
mdropout <- getModelInfo("mlpKerasDropout")
knitr::kable(mdropout$mlpKerasDropout$parameters,caption="Hiperparámetros para el algoritmo MLP con dropout") %>% 
  kable_styling(bootstrap_options = "condensed")
```

* `size`: número de nodos.
* `dropout`: ratio de dropout. El dropout implica que se ignoren neuronas seleccionadas al azar durante el entrenamiento. Se "abandonan" aleatoriamente.
* `batch_size`: el tamaño de cada lote antes de actualizar los pesos de la red. Por defecto se toma un tercio de los ejemplares del conjunto que se utiliza para el training.
* `lr`: el ratio de aprendizaje.
* `rho`: atenúa la influencia del gradiente pasado. Es similar al impulso y se relaciona con la memoria de las actualizaciones de peso anteriores.
* `decay`: decaimiento de la tasa de aprendizaje
* `activation`: tipo de activación a la salida. Puede ser: `sigmoid`, `relu` y `tanh`.

Los hiperparámetros escogidos finalmente para generar este modelo fueron: `size = 12`, `dropout = 0.1`, `batch_size = 5`, `lr = (0.5*1e-2)`,`rho = 0`,`decay = 0` y `activation = "relu"`. Con estos hiperparámetros se consigue la mejor optimización probada.

El parámetro `epoch` (épocas) va a tener un valor de 10 El número de épocas es un hiperparámetro que define el número de veces que el algoritmo de aprendizaje funcionará en todo el conjunto de datos de entrenamiento. Una época significa que cada muestra en el conjunto de datos de entrenamiento ha tenido la oportunidad de actualizar los parámetros del modelo interno. Se suele seleccionar un valor de 10 para este hiperparámetro.

De nuevo se usa *cross-validation* con 10 folders.

En este caso no se utilizó multicore para agilizar la ejecución del proceso porque resulta contraproducente su uso cuando se emplea este algoritmo de Deep learning.

```{r, eval=FALSE}
# Deep learning de datos de entrenamiento balanceados con downSample
Sys.setenv(RETICULATE_PYTHON="/usr/bin/python3.8")

set.seed(123)

deep_learning_control = trainControl(method="cv",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 12, 
                     dropout = 0.1, 
                     batch_size = 5,
                     lr = (0.5*1e-2),
                     rho = 0,
                     decay = 0,
                     activation = "relu")

if (file.exists("deeplearning_downSample")) {
    deeplearning_downSample <- readRDS("deeplearning_downSample")
} else {
    deeplearning_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, preProcess=c("scale","center"), 
        tuneGrid=mygrid, trControl=deep_learning_control, method="mlpKerasDropout", epochs=10)
    saveRDS(deeplearning_downSample, "deeplearning_downSample")
}
```

```{r}
# Resultados de deep learning con datos balanceados con downSample
deeplearning_downSample <- readRDS("./deeplearning_downSample")
deeplearning_downSample
```

Se procedió entonces a implementar el mismo algoritmo usando los datos balanceados mediante `downSample`.

```{r, eval=FALSE}
# Deep learning de datos de entrenamiento balanceados con upSample
set.seed(123)

deep_learning_control = trainControl(method="cv",
                                     number=10,
                                     search = "grid",
                                     classProbs = T)

mygrid = expand.grid(size = 12, 
                     dropout = 0.1, 
                     batch_size = 5,
                     lr = (0.5*1e-2),
                     rho = 0,
                     decay = 0,
                     activation = "relu")


if (file.exists("deeplearning_upSample")) {
  deeplearning_upSample <- readRDS("deeplearning_upSample")
} else {
  deeplearning_upSample <- caret::train(Class ~ ., data = datos3UTRtrain_up, preProcess=c("scale","center"), tuneGrid=mygrid, 
                                         trControl=deep_learning_control, method="mlpKerasDropout", epochs=10)
  saveRDS(deeplearning_upSample, "deeplearning_upSample")
}
```

```{r}
# Resultados de deep learning con datos balanceados con upSample
deeplearning_upSample <- readRDS("./deeplearning_upSample")
deeplearning_upSample
```

Para el caso del modelo de Deep learning con los datos balanceados mediante `downSample`, se obtiene como resultado una *Accuracy* de 93% y un valor de *Kappa* de 87% frente a los resultados obtenidos en el caso del modelo de Deep learning con los datos balanceados mediante `upSample`, en el que se obtiene un *Accuracy* de 94% y un valor de *Kappa* del 87%. 

En este caso, ambos modelos tienen resultados muy similares y se puede decir que **el método de balanceo de los datos de ambas formas logra para este algoritmo resultados semejantes en cuanto a las métricas de *accuracy* y *kappa* para este conjunto de datos de estudio y este problema particular**.

## Comparación de modelos

Después de realizar todos los modelos, se va a proceder a su **comparación**. Para ello, se van a realizar dos tipos de comparaciones:

* Se va a comparar los modelos de `Ranger forest` y `Deep learning` **dependiendo del tipo de balanceo de sus datos**: `downSample` y `upSample`. Estos dos modelos se han podido realizar con ambos tipos de datos y se va a proceder a su comparación con el fin de comprobar de entre los tipos de balanceo, cuál es el mejor resultado en ambos modelos.

* Se van a comparar **los modelos de Ranger forest y Deep learning, que son los dos modelos que mejores resultados han arrojado, con los datos balanceados mediante `downSample`** con el objetivo de comprobar cuál es el mejor modelo de entre los dos.

Para poder proceder a la comparación de los modelos, se va a generar la **matriz de confusión** de cada uno de ellos, usando la función `confusionMatrix`. En dichas matrices de confusión se comparan las predicciones de los modelos con sus valores reales, y permite la visualización del desempeño de un algoritmo que se emplea en aprendizaje supervisado [@visa2011confusion]. 

Estas **predicciones se van a realizar sobre el subconjunto de test de los datos**, utilizando para ello la función `predict`.

### Ranger forest y Deep learning con datos undersampling y oversampling

#### Ranger forest con datos downSampleados y upSampleados

Primero se va a usar la función `predict` para predecir nuevos datos. Esta función utiliza los datos generados previamente por la función `train()`. En concreto, la función `predict()` utiliza el modelo almacenado en *$finalModel*, que es el mejor modelo de entre los ajustados en el proceso de optimizacióna. 

Tras esto, se van a generar las **matrices de confusión**:

```{r}
# Ranger forest con datos balanceados con downSample
prediccion_ranger_down <- predict(rangerforest_downSample, datos3UTR_test, type = "raw")
confusion_ranger_down <- confusionMatrix(prediccion_ranger_down, datos3UTR_test$class)
resultados_confusion_ranger_down <- as.data.frame(t(confusion_ranger_down$byClass))
matriz_confusion_ranger_down <- as.data.frame(confusion_ranger_down$table)

# Ranger forest con datos balanceados con upSample
prediccion_ranger_up <- predict(rangerforest_upSample, datos3UTR_test, type = "raw")
confusion_ranger_up <- confusionMatrix(prediccion_ranger_up, datos3UTR_test$class)
resultados_confusion_ranger_up <- as.data.frame(t(confusion_ranger_up$byClass))
matriz_confusion_ranger_up <- as.data.frame(confusion_ranger_up$table)
```

A continuación, se muestran los resultados obtenidos en las matrices de confusión:

```{r, echo = FALSE}
# Resultados de matrices de confusión de Ranger forest para datos balanceados con downSample
resultados_rangerdown <- table(resultados_confusion_ranger_down)
knitr::kable(resultados_rangerdown, caption="Resultados Ranger forest - downSample") %>% kable_styling(bootstrap_options = "condensed")
```
```{r, echo = FALSE}
# Resultados de matrices de confusión de Ranger forest para datos balanceados con upSample
resultados_rangerup <- table(resultados_confusion_ranger_up)
knitr::kable(resultados_rangerup, caption="Resultados Ranger forest - upSample") %>% kable_styling(bootstrap_options = "condensed")
```
Si se comparan los resultados obtenidos tras el la generación de las matrices de confusión de Ranger forest para el caso de los datos balanceados mediante `downSample` y `upSample` se puede ver que hay claras diferencias en la sensibilidad y especificidad de ambos modelos.

Como se sabe, la sensibilidad es la fracción de verdaderos positivos y la especificidad la fracción de verdaderos negativos. Los resultados obtenidos de ambas matrices de confusión muestran que **el modelo en el caso de los datos balanceados con `downSample` tiene una mayor sensibilidad, es decir, capacidad de identificar a los verdaderos positivos, pero una menor especificidad, es decir, capacidad de distinguir a los verdaderos negativos, que en el caso del modelo con datos balanceados con `upSample`**.

A continuación, se representan las matrices de confusión:

```{r}
# Matriz de confusión ranger forest con datos balanceados con downSample
plot_matriz_ranger_down <- ggplot2:::ggplot.default(matriz_confusion_ranger_down, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("RandomForests - downSample")

# Matriz de confusión ranger forest con datos balanceados con upSample
plot_matriz_ranger_up <- ggplot2:::ggplot.default(matriz_confusion_ranger_up, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("RandomForest - upSample")

grid.arrange(plot_matriz_ranger_down, plot_matriz_ranger_up, ncol = 2)
```

Como se puede observar, al **comparar la matriz de confusión de Ranger forest con datos balanceados con `downSample` y Ranger forest con datos balanceados con `upSample`**, se puede apreciar que **el modelo en el que se han usado los datos balanceados con `upSample` da unos mejores resultados a nivel general**: predice mucho mejor las regiones NO 3-UTR y tiene menos fallo de predicción en cuanto a predecir 3-UTR cuando en realidad no lo es, aunque generalmente se ve que el modelo de Ranger forest con los datos balanceados con `downSample` predice mejor las regiones sí 3-UTR, como se comentó anteriormente en los resultados de las matrices de confusión.

El siguiente paso a realizar en esta comparación de modelos sería **comparar utilizando la herramienta `resample` los resultados obtenidos en los distintos folds de los modelos**. 

Para realizar esto, se introducen los modelos en la función y se muestran los resultados mediante un gráfico de cajas y bigotes y otro de dispersión:

```{r}
# Resample para RF con datos balanceados con downSample y upSample
modelos_rf <- list(RF_down = rangerforest_downSample, RF_up = rangerforest_upSample)
resample_modelos_rf <- resamples(modelos_rf, quadratic_model = NULL)
plot1_rf <- bwplot(resample_modelos_rf, main = "Modelos Random forest")
plot2_rf <- xyplot(resample_modelos_rf)

grid.arrange(plot1_rf, plot2_rf, ncol = 2)
```

En la primera de las gráficas se puede ver, como se ha dicho, una gráfica de cajas y bigotes en la que se **compara la distribución de la precisión de los modelos Ranger forest con los datos balanceados con `downSample` y con `upSample` y su valor de accuracy y kappa**, siendo este último valor más fiable porque tiene en cuenta la implicación del azar a la hora de realizar la clasificación.

En este caso se puede apreciar que el valor de *kappa* de Ranger forest en el caso en el que se han balanceado los datos con `upSample` posee una menor varianza y unos valores mejores que el caso del Ranger forest que se ha hecho con los datos balanceados con el método de `downSample`. También se puede observar que hay menos dispersión en el caso de Rf_up que en el caso de Rf_down.

Se pasa a observar ahora el gráfico de dispersión de la derecha, que muestra en cada uno de los folds del Random forest qué modelo es el que mejor *accuracy* da. Si los valores quedan por debajo de la diagonal, quiere decir que los resultados de RF usando los datos balanceados con `upSample` tiene unos mejores estadísticos, mientras que si quedan por encima, significará que pasa lo contrario y que los resultados de RF usando los datos balanceados con `downSample` son los que tienen mejores estadísticos.

Analizando el gráfico, se observa que **los valores de del RF en el que se han usado los datos balanceados con `upSample` son los que muestran unos mejores valores estadísticos**.

#### Deep learning con datos downSampleados y upSampleados

Se repite el proceso anterior para el caso de los modelos de Deep learning con los datos balanceados con `downSample` y `upSample`. Primero se utiliza la función `predict` y se generan las matrices de confusión y los resultados de estas matrices:

```{r, warning=FALSE, echo=FALSE}
Sys.setenv(RETICULATE_PYTHON="/usr/bin/python3.8")
# Deep learning con datos balanceados con downSample
prediccion_deep_down <- predict(deeplearning_downSample, datos3UTR_test, type = "raw")
confusion_deep_down <- confusionMatrix(prediccion_deep_down, datos3UTR_test$class)
resultados_confusion_deep_down <- as.data.frame(t(confusion_deep_down$byClass))
matriz_confusion_deep_down <- as.data.frame(confusion_deep_down$table)

# Deep learning con datos balanceados con upSample
prediccion_deep_up <- predict(deeplearning_upSample, datos3UTR_test, type = "raw")
confusion_deep_up <- confusionMatrix(prediccion_deep_up, datos3UTR_test$class)
resultados_confusion_deep_up <- as.data.frame(t(confusion_deep_up$byClass))
matriz_confusion_deep_up <- as.data.frame(confusion_deep_up$table)
```

A continuación, se muestran los resultados obtenidos en **las matrices de confusión**:

```{r, echo=FALSE}
# Resultados de matrices de confusión de Deep learning para datos balanceados con downSample
resultados_deepdown <- table(resultados_confusion_deep_down)
knitr::kable(resultados_deepdown, caption="Resultados Deep learning - downSample") %>% kable_styling(bootstrap_options = "condensed")
```

```{r, echo=FALSE}
# Resultados de matrices de confusión de Deep learning para datos balanceados con upSample
resultados_deepup <- table(resultados_confusion_deep_up)
knitr::kable(resultados_deepup, caption="Resultados Deep learning - upSample") %>% kable_styling(bootstrap_options = "condensed")
```

Si se comparan los resultados obtenidos tras el la generación de las matrices de confusión de Deep learning para el caso de los datos balanceados mediante `downSample` y `upSample`, se pueden apreciar que no hay casi diferencias entre ambas.

En el caso del modelo de Deep learning con datos balanceados con `downSample`, se obtiene unos valores de sensibilidad y especificidad de 91,2% y 95,6% respectivamente, mientras que en el caso del modelo Deep learning con datos balanceados con `upSample` se obtienen valores de 91,3% y 95,7%.

**En cuanto a valores de sensibilidad y especificidad, es ligeramente mejor el modelo de Deep learning con datos balanceados con `upSample`, encontrando también que la precisión del modelo de este último es también mayor.**

A continuación, se representan las matrices de confusión:

```{r, echo=FALSE}
# Matriz de confusión deep learning con datos balanceados con downSample
plot_matriz_deep_down <- ggplot2:::ggplot.default(matriz_confusion_deep_down, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("Deep learning - downSample")

# Matriz de confusión deep learning con datos balanceados con upSample
plot_matriz_deep_up <- ggplot2:::ggplot.default(matriz_confusion_deep_up, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("Deep learning - upSample")

grid.arrange(plot_matriz_deep_down, plot_matriz_deep_up, ncol = 2)
```

Al **comparar la matriz de confusión de Deep learning con datos balanceados con `downSample` y Deep learning con datos balanceados con `upSample`**, se puede apreciar que **el modelo en el que se han usado los datos balanceados con `upSample` da unos mejores resultados en todos los aspectos**: predice mejor que el modelo Deep learning con datos `downSample` las regiones sí 3-UTR y no 3-UTR, y tiene menos margen de error al detectar regiones 3-UTR cuando no lo son, así como también falla menos al detectar regiones No 3-UTR cuando en realidad no lo son. 

El siguiente paso sería comparar utilizando la herramienta `resample` los resultados obtenidos en los distintos folds de los modelos. 

Para realizar esto, se introducen los modelos en la función y se muestran los resultados mediante un gráfico de cajas y bigotes y otro de dispersión:

```{r, echo=FALSE}
# Resample para DL con datos balanceados con downSample y upSample
modelos_dl <- list(DL_down = deeplearning_downSample, DL_up = deeplearning_upSample)
resample_modelos_dl <- resamples(modelos_dl, quadratic_model = NULL)
plot1_dl <- bwplot(resample_modelos_dl, main = "Modelos Deep learning")
plot2_dl <- xyplot(resample_modelos_dl)

grid.arrange(plot1_dl, plot2_dl, ncol = 2)
```

En la primera de las gráficas se puede ver una gráfica de cajas y bigotes en la que se **compara la distribución de la precisión de los modelos Deep learning con los datos balanceados con `downSample` y con `upSample` y su valor de accuracy y kappa**, siendo este último valor más fiable porque tiene en cuenta la implicación del azar a la hora de realizar la clasificación.

En este caso se puede apreciar que el valor de *kappa* de Deep learning en el caso en el que se han balanceado los datos con `upSample` posee una menor varianza y unos valores mejores que el caso del Deep learning que se ha hecho con los datos balanceados con el método de `downSample`. También se puede observar que hay menos dispersión en el caso de DL_up que en el caso de DL_down.

Se pasa a observar ahora el gráfico de dispersión de la derecha, que muestra en cada uno de los *folds* del Deep learning qué modelo es el que mejor *accuracy* da. Si los valores quedan por encima de la diagonal, quiere decir que los resultados de DL usando los datos balanceados con `downSample` tiene unos mejores estadísticos, mientras que si quedan por debajo, significará que pasa lo contrario y que los resultados de DL usando los datos balanceados con `upSample` son los que tienen mejores estadísticos.

Analizando el gráfico, se observa que **los valores de del DL en el que se han usado los datos balanceados con `upSample` son los que muestran unos mejores valores estadísticos**.

### Comparación de modelos Ranger forest y Deep learning con datos undersampling

A continuación se va a proceder a comparar los **dos mejores modelos seleccionados: el Ranger forest y el modelo Deep learning generado**, con los datos balanceados mediante el método de downSample.

Previamente, durante el estudio, se implementaron otros modelos que NO dieron los mismos resultados que los expuestos para el caso de Ranger forest y de Deep learning. La generación de estos modelos y sus resultados se puede encontrar en el apartado de **Modelos descartados**

Se va a proceder entonces a comparar estos dos modelos entre sí con el objetivo de concretar cuál de ellos es el mejor modelo de todos los generados con los datos balanceados mediante `downSample`.

Como se ha visto previamente, lo primero que se va a hacer es **usar la función `predict` para predecir nuevos datos, y tras esto, generar las matrices de confusión así como los resultados de dichas matrices de confusión**. 

Los resultados de las matrices de confusión se muestran a continuación:

```{r, echo=FALSE}
# Resultados de matrices de confusión de Ranger forest para datos balanceados con downSample
resultados_rangerdown <- table(resultados_confusion_ranger_down)
knitr::kable(resultados_rangerdown, caption="Resultados Ranger forest - downSample") %>% kable_styling(bootstrap_options = "condensed")
```

```{r, echo=FALSE}
# Resultados de matrices de confusión de Deep learning para datos balanceados con downSample
resultados_deepdown <- table(resultados_confusion_deep_down)
knitr::kable(resultados_deepdown, caption="Resultados Deep learning - downSample") %>% kable_styling(bootstrap_options = "condensed")
```

Como se puede comprobar en las tablas expuestas anteriormente, se obtiene, para el caso de los resultados de la matriz de confusión de Ranger forest con datos balanceados con `downSample` una sensibilidad y especificidad del 91,7% y 95,7% respectivamente, mientras que en el caso de los resultados de la matriz de confusión de Deep learning con datos balanceados con `downSample` encontramos valores de sensibilidad y especificidad de 91,2% y 95,6% respectivamente.

**Los resultados son ligeramente mejores en el caso de Ranger forest que en Deep learning, pero la diferencia es muy pequeña**.

A continuación, se muestran las **matrices de confusión** de los dos modelos:

```{r, echo=FALSE}
# Matriz de confusión Ranger forest con datos balanceados con downSample
plot_matriz_ranger_down <- ggplot2:::ggplot.default(matriz_confusion_ranger_down, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("RandomForests - downSample")

# Matriz de confusión Deep learning con datos balanceados con downSample
plot_matriz_deep_down <- ggplot2:::ggplot.default(matriz_confusion_deep_down, aes(Reference, 
    Prediction)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(low = "white", 
    high = "pink") + theme_bw() + coord_fixed() + geom_text(aes(label = Freq)) + 
    theme(legend.position = "none", plot.title = element_text(size = 10, 
        face = "bold")) + ggtitle("Deep learning - downSample")

grid.arrange(plot_matriz_ranger_down, plot_matriz_deep_down, ncol = 2)
```

Como se puede apreciar en las matrices de confusión expuestas anteriormente para el caso del modelo Ranger fores y Deep learning, ambos con los datos balanceados mediante el método `downSample`, se aprecia que **el modelo Ranger forest muestra unos mejores resultados que el Deep learning a la hora de predecir regiones 3-UTR**, así como para predecir regiones que no son 3-UTR.

Además de eso, presenta un menor fallo a la hora de predecir regiones como 3-UTR cuando no lo son, así como también para predecir regiones no 3-UTR cuando no lo son, por lo que en general, **el modelo de Ranger forest predice mejor que el modelo de Deep learning para este estudio particular.**


El siguiente paso sería comparar utilizando la herramienta `resample` los resultados obtenidos en los distintos folds de los modelos. 

Para realizar esto, se introducen los modelos en la función y se muestran los resultados mediante un gráfico de cajas y bigotes y otro de dispersión.

```{r, echo=FALSE}
# Resample de los modelos con datos balanceados con downSample 
modelos <- list(RF=rangerforest_downSample, 
                 DL=deeplearning_downSample)

resample_modelos <- resamples(modelos)

plot1 <- bwplot(resample_modelos, main = "RF vs DL")
plot2 <- xyplot(resample_modelos)

grid.arrange(plot1, plot2, ncol = 2)
```

En la primera gráfica se puede ver una gráfica de cajas y bigotes en la que se **compara la distribución de la precisión del (Ranger forest) y del modelo DL (Deep learning), ambos con los datos balanceados con `downSample`, y su valor de Accuracy y Kappa**, siendo este último valor más fiable porque tiene en cuenta la implicación del azar a la hora de realizar la clasificación.

En este caso se puede apreciar que el valor de *kappa* de Ranger forest posee una menor varianza y unos valores mejores que el caso del Deep learning. También se puede observar que hay menos dispersión en el caso de RF que en el caso de DL.

Se pasa a observar ahora el gráfico de dispersión de la derecha, que muestra en cada uno de los *folds* qué modelo es el que mejor *accuracy* da. Si los valores quedan por encima de la diagonal, quiere decir que los resultados de Ranger forest tiene unos mejores estadísticos, mientras que si quedan por debajo, significará que pasa lo contrario y que los resultados de Deep learning son los que tienen mejores estadísticos.

Analizando el gráfico, se observa que **los valores de Ranger forest son los que muestran unos mejores valores estadísticos, ya que hay una mayor presencia en la zona en la que están representados**.


Tras esto, se va a proceder a obtener los **valores estadísticos que van a permitir analizar si hay diferencias significativas entre el modelos** estudiados previamente. 

Para ello, lo primero que se va a hacer, es comprobar si hay **normalidad en los datos**. Para saber esto se se va a utilizar el *test de Shapiro Wilk*, que tiene como hipótesis nula la normalidad de los datos. Usa los distintos valores de *Kappa*:

```{r}
# Prueba de Shapiro-Wilk para Ragner forest - downSample
shapiro.test(resample_modelos$values$`RF~Kappa`)
```

```{r}
# Prueba de Shapiro-Wilk para Deep Learning - downSample
shapiro.test(resample_modelos$values$`DL~Kappa`)
```
Como se puede comprobar en ambos casos, se obtiene un p-valor por encima de 0.05, por lo que no **se rechaza la hipótesis nula y se asume la normalidad de los datos** en el caso de Ranger forest y Deep learning con los datos balanceados mediante `downSample`.

Una vez comprobada la normalidad, hay que proceder con la **comprobación de la homocedasticidad** (homogeneidad de las varianzas). Para ello se cuenta con dos test: el *test de Levene* y el *test de Bartlett*. 

Este último es más recomendable en caso de que se pueda asumir la normalidad, como pasa en este estudio:

```{r}
# Prueba de homogeneidad de las varianzas para el caso de RF y DL - downSample
datos <- stack(list(rf=resample_modelos$values$`RF~Kappa`,dl=resample_modelos$values$`DL~Kappa`))
test.barlett <- bartlett.test(values ~ ind, data = datos)
test.barlett
```

Como se puede comprobar, el p-valor obtenido es superior a 0.05, por lo que no se rechaza la hipótesis nula y se asume **la homogeneidad de las varianzas en el caso de Ranger forest y Deep learning con los datos balanceados mediante `downSample`**.

Una vez comprobada la normalidad de los datos y la homogeneidad de las varianzas, se puede realizar el test `diff`, que permite comparar las medias de muestras apareadas. 

Esta función `diff` aplica por defecto la prueba T de Student pareada con ajuste de *Bonferroni* y determina si las diferencias detectadas entre los modelos son estadísticamente significativas. Es por esto que se necesita comprobar previamente la normalidad de los datos y la homogeneidad de las varianzas

`Diff` utiliza como parámetros de entrada los datos del resample, y hay que especificar que no se quiere que se haga ningún ajuste o modificación sobre los mismos (`adjustment = "none"`). 

```{r}
# Se aplica la función `diff` sobre el resample resultante de los modelos Ranger forest y Deep learning
diffValues <- diff(resample_modelos, adjustment = "none")

# Summary de estadísticas resultantes
summary(diffValues)
```
En los resultados de esta función `diff`, los valores que hay por encima de la diagonal expresan la diferencia de las medias en valor absoluto, mientras que los valores por debajo de la diagonal indican el p-valor de que esta diferencia sea significativa.

Como se puede comprobar en los resultados, la diferencia de la media de *kappa* de los modelos de Ranger forest y de Deep learning es de 0.007902, y el p-valor de esta diferencia es de 0.01902. Al ser este p-valor menor de 0.05 se puede determinar que **sí que hay diferencias significativas entre ambos modelos**.

**Se rechaza entonces la hipótesis nula de que la diferencias de las medias es cero tanto para el *Accuracy* como el *Kappa*, por lo que se determina que las diferencias apreciadas son significativas, indicando que el clasificador Ranger Forest es el mejor de los dos para predecir en este estudio y con los datos presentes en el mismo**.

A continuación se muestra gráficamente esta diferencia significativa entre ambos modelos:

```{r}
# Levelplot para diferencias significativas entre RF vs DL - downSample
levelplot(diffValues, what="diferences", main="Diferencias significativas entre RF y DL")
```

En la representación gráfica anterior se puede comprobar que **al enfrente los valores de *Accuracy* de los modelos Ranger forest y Deep learning se obtienen diferencias significativas entre ambos modelos**, ya que si no existieran, el gráfico adoptaría, al enfrentar los modelos, una tonalidad blanca, que simboliza el cero en el *levelplot* e implicaría la no existencia de diferencias significativas entre los dos modelos.

Sin embargo, adquiere una tonalidad rosácea de la que se puede extraer que las diferencias sí son significativas.

## Features/predictores más relevantes en el estudio

Una vez se ha determinado cuál es el mejor modelo de entre los especificados en el estudio, se puede proceder a la comprobación de cuáles son los predictores más importantes para la predicción de las regiones 3-UTR en este estudio particular y para estos datos.

El mejor modelo de los estudiados es Ranger forest, **pero en el modelo Ranger forest generado en este estudio NO es posible identificar cuáles son los predictores más importantes**. En el modelo en el que **sí resulta posible es en el de `rf`**. 

Se ha seleccionado este modelo debido a que presenta unos resultados muy similares a los obtenidos para el caso de Ranger forest, con una accuracy de 93,9% y un valor de kappa de 87,9%, siendo este modelo el mejor, junto a SVM, de los modelos descartados que se probaron al realizar el estudio.

```{r}
# Resultados de Random forest `rf` para datos balanceados con - downSample
rf_downSample <- readRDS("./rf_downSample")
rf_downSample
```

Se procede ahora a mostrar los predictores más importantes para el modelo de `rf` realizado con los datos balanceados mediante `downSample`.

```{r}
# Predictores más importantes en Random forest - downSample
varImp(rf_downSample)
```

Como se puede comprobar, los predictores más importantes para este estudio son `polyA_signal1`, `meanPd`, `meanEE` y `mean_phastCons7way`.

En el apartado de *Discusión* se pasará a observar cada uno de estos predictores y a comprobar si se puede extraer alguna explicación del por qué de su implicación biológica a la hora de identificar regiones 3-UTR.

## Modelos descartados

A continuación se muestran una serie de modelos que se implementaron pero que se descartaron por dar unos peores resultados que los seleccionados para la comparación de modelos.

### rf

Se procedió a realizar un Random forest usando el algoritmo `rf` de `Caret`. 

A continuación se muestra información de este algoritmo.

```{r}
# Información sobre algoritmo `rf`
getModelInfo("rf")$rf$grid
```
Como se puede comprobar, este algoritmo sólo permite la modificación del parámetro `mtry`.

```{r}
# Parámetros que requiere un rf
rf <- table(modelLookup("rf"))
knitr::kable(rf, caption="Hiperparámetros que requiere un rf") %>% kable_styling(bootstrap_options = "condensed")
```

Se decidió implementar el mismo algoritmo que en el caso de Ranger forest, es decir, los mismos ajustes, sólo variando el método utilizado a `rf`.

```{r}
# Rf con datos balanceados mediante downSample
set.seed(123)
random_forest_control <- trainControl(method = "cv", number = 10, 
                                      returnResamp = "final", verboseIter = T, allowParallel = T)

set.seed(123)
if (file.exists("rf_downSample")) {
  rf_downSample <- readRDS("rf_downSample")
} else {
  rf_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, method = "rf", tuneLength = 10, trControl = random_forest_control)
  saveRDS(rf_downSample, "rf_downSample")
}
```

```{r}
# Resultados de rf para datos balanceados mediante downSample
rf_downSample
```

```{r}
# Predictores importantes para el modelo rf con datos balanceados con downSampl
var_imp <- varImp(rf_downSample)
```

Este modelo permite comprobar cuáles son los predictores más importantes para predecir.

Como se puede apreciar, los resultados son ligeramente peores que en el caso de `Ranger forest`. Esto es esperable, ya que este algoritmo de `rf` sólo permite la especificación de un único hiperparámetro, y presenta un menor nivel de complejidad que en el caso de Ranger forest.

### mlpKerasDropout con "boot"

Se decidió probar a generar un modelo de Deep learning usando también el algoritmo de `mlpKerasDropout` pero usando el método "boot". Se utiliza para aproximar la distribución en el muestreo de un estadístico. Se usa frecuentemente para aproximar el sesgo o la varianza de un análisis estadístico [@little2019causal].

Esta función utiliza *Dropout* para evitar el *overfitting* del modelo, ya que este fenómeno dificulta la precisión y el rendimiento del modelo. Se dice que un modelo estadístico está sobreajustado cuando se le entrena con muchos datos. Cuando un modelo se entrena con muchos datos, comienza a aprender del ruido y las entradas de datos inexactas en el conjunto de datos. Esto provoca que el modelo no categorice los datos correctamente, debido a demasiados detalles y ruido [@ying2019overview].

Esta función va a ir eliminando de forma aleatoria determinados nodos en cada uno de los distintos pasos, estudiando su efecto en el resultado final, por lo que gracias a ello no se va a producir el fenómeno de sobreajuste expuesto con anterioridad.

```{r}
Sys.setenv(RETICULATE_PYTHON="/usr/bin/python3.8")
set.seed(123)

deep_learning_control = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 12, 
                     dropout = 0.1, 
                     batch_size = 5,
                     lr = (0.5*1e-2),
                     rho = 0,
                     decay = 0,
                     activation = "relu")

if (file.exists("deeplearning_downSample")) {
    deeplearning_downSample <- readRDS("deeplearning_downSample")
} else {
    deeplearning_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, preProcess=c("scale","center"), 
        tuneGrid=mygrid, trControl=deep_learning_control, method="mlpKerasDropout", epochs=10)
    saveRDS(deeplearning_downSample, "deeplearning_downSample")
}
```

```{r}
# Resultados para deep learning con método boot
print(deeplearning_downSample)
```

Aunque da los mismos resultados que el modelo de Deep learning escogido, que es el que usó el método de `cv`, se decidió implementar este último para permitir una mejor comparación en el resampleo de los datos, por lo que este se descartó.

### nb

El **algoritmo de Naive Bayes** calcula las probabilidades condicionales de que una observación pertenezca a cada una de las clases dados siempre unos valores de los predictores [@lewis1998naive]. El término naive asume que las variables son independientes, es decir, que el valor que toman no está influenciado por las variables. Aunque en la práctica es difícil que se de esto, este algoritmo permite calcular la probabilidad cuando hay múltiples predictores multiplicando las probabilidades individuales de cada uno (regla de eventos independientes)[@lewis1998naive].

Este algoritmo de Naive Bayes se puede implementar con la función `nb` de `Caret`. 

Se procede a presentar información sobre este algoritmo, el cómo se implementa y sus valores por defecto:

```{r}
# Información sobre algoritmo `nb`
getModelInfo("nb")$nb$grid
```

Como se ve en la ejecución anterior, los valores por defecto de los parámetros del algoritmo son: `usekernel = c(TRUE, FALSE)`, `fL = 0`, `adjust = 1`.

Este algoritmo de Naive Bayes tiene tres hiperparámetros:

```{r}
# Hiperparámetros para el algoritmo `nb` de Naive Bayes
nb <- getModelInfo("nb")
knitr::kable(nb$nb$parameters, caption="Hiperparámetros para el algoritmo `nb` de Naive Bayes") %>% 
  kable_styling(bootstrap_options = "condensed")
```


* `usekernel`: utiliza TRUE para emplear un *kernel* que estime la densidad o FALSE para asumir una distribución de densidad gaussiana.
* `fL`: factor de corrección de Laplace. Se usará 0 en caso de no aplicar ninguna corrección.
* `adjust`: parámetro pasado a la función density si `usekernel = TRUE`.

Se hizo uso de nuevo de `tuneGrid` para la especificación de los parámetros. En este caso, tras algunas pruebas, se decidió que el mejor modelo se obtenía aplicando los valores por defecto de los hiperparámetros.

Se hará uso del método `cv` como se hizo anteriormente con otros modelos. También se especificará que tome como métrica `metric = "Accuracy"`.

```{r}
# Naive Bayes con datos balanceados con downSample
set.seed(123)

hiperparametros <- data.frame(usekernel = FALSE, fL = 0 , adjust = 0)

naive_bayes_control_train <- trainControl(method = "cv", number = 10,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

set.seed(123)
if (file.exists("naive_bayes_downSample")) {
  naive_bayes_downSample <- readRDS("naive_bayes_downSample")
} else {
  naive_bayes_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, method = "nb", 
                                       tuneGrid = hiperparametros, trControl = naive_bayes_control_train)
  saveRDS(naive_bayes_downSample, "naive_bayes_downSample")
}

# Resultados:
naive_bayes_downSample
```

```{r}
# Resultados de Naive Bayes para datos balanceados con downSample
naivebayes_down_resultados <- table(naive_bayes_downSample$results)

knitr::kable(naivebayes_down_resultados, caption="Resultados de Naive Bayes con datos balanceados con downSample") %>% kable_styling(bootstrap_options = "condensed")
```

Empleando un modelo Naive Bayes con `usekernel = FALSE`, `fL = 0`, `adjust = 0`, se consigue un *accuracy* promedio de **validación del 89%** para el caso de los datos balanceados mediante downSample. El valor de *kappa* alcanzado sería de un 79%.

En general este algoritmo presenta peores valores de *accuracy* que los modelos de Ranger forest y de Deep learning, por lo que se decidió no compararlo entre estos anteriores, aunque resultó interesante probar este modelo para comprobar los valores que reflejaba.

### regLogistic

Se procedió a realizar un modelo de **Regresión logística**, usando la función `regLogistic` de `Caret`.

A continuación se procede a mostrar información de este algoritmo, cómo se implementa y los valores de sus hiperparámetros por defecto:

```{r}
# Información sobre algoritmo `regLogistic`
getModelInfo("regLogistic")$regLogistic$grid
```

Como se puede apreciar en la ejecución anterior, se muestran los valores por defecto que toman los hiperparámetros de este algoritmo, así como la forma de implementación del mismo: `(cost = 2^((1:len) - ceiling(len * 0.5)), loss = c("L1", "L2_dual", "L2_primal"), epsilon = signif(0.01 * (10^((1:len) - ceiling(len * 0.5))), 2))`

A continuación se muestran los hiperparámetros que posee dicho algoritmo:

```{r}
# Hiperparámetros para algoritmo `regLogistic` de Regresión logística
regLogistic <- getModelInfo("regLogistic")
knitr::kable(regLogistic$regLogistic$parameters, caption="Hiperparámetros para el algoritmo `regLogistic` de Regresión logística") %>% kable_styling(bootstrap_options = "condensed")
```

Como se puede apreciar, dos de ellos son numéricos y uno es de tipo *character* o caracter.

* `cost`: cuantifica el error entre los valores predichos y los valores esperados.
* `loss`: registro de probabilidades predichas corregidas para cada instancia.
* `epsilon`: tolerancia registrada.

En este caso se hizo uso de nuevo del parámetro `tuneLength`=10 con el objetivo de generar los modelos y que finalmente se seleccionara el mejor, por lo que en este caso no se realizaron manualmente modificaciones de los hiperparámetros.

De nuevo se utilizará el método de `cv` con un número de 10. Se hará uso de la función `tuneLength` con un valor igual a 10, como se hizo en los modelos de Ranger forest.

```{r}
# Regresión logística con datos balanceados con downSample
set.seed(123)

regresion_logistica_control_train  <- trainControl(method = "cv", number = 10,
                                                   returnResamp = "final", verboseIter = FALSE,
                                                   allowParallel = TRUE)

if (file.exists("reg_log_downSample")) {
  reg_log_downSample <- readRDS("reg_log_downSample")
} else {
  reg_log_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, method = "regLogistic", 
                                                 tuneLength = 10, trControl = regresion_logistica_control_train)
  saveRDS(reg_log_downSample, "reg_log_downSample")
}

# Resultados
reg_log_downSample
```

A continuación se muestran los mejores resultados de los hiperparámetros para el caso de la regresión logística con datos balanceados con downSample:

```{r}
# Resultados de reg logística para datos balanceados con downSample 
reg_log_down_resultados <- table(reg_log_downSample$bestTune)

knitr::kable(reg_log_down_resultados, caption="Resultados de Reg Logística con datos balanceados con downSample") %>% kable_styling(bootstrap_options = "condensed")
```

A continuación se muestran los valores de *Accuracy* y *Kappa* del mejor modelo obtenido y el modelo final:

```{r}
# Mejor resultado obtenido
reg_logistica_mejor <- table(reg_log_downSample$results[1,])
knitr::kable(reg_logistica_mejor, caption="Mejor resultado de Reg Logística con datos balanceados con downSample") %>% kable_styling(bootstrap_options = "condensed")
```

```{r}
# Modelo final de regLogistic 
reg_log_downSample$finalModel
```

Como se puede apreciar, se obtiene una *Accuracy* del 93% y un valor de *Kappa* del 86%, por lo que da unos muy buenos resultados para el caso de los datos balanceados con downSample. 

En cuanto al modelo final, el mejor ajuste de los hiperparámetros para el mejor modelo es: `cost = 32`, `loss = L1` Y `epsilon = 1e-06`.

Aunque el modelo da muy buenos resultados, sigue siendo inferior al modelo tanto de Ranger forest como de Deep learning y por eso se decidió no compararlo con estos últimos.

### SVM

El método de **Máquina de Vector Soporte** o algoritmo `svmRadial` de `Caret` permiten encontrar la forma óptima de clasificar entre varias clases. La clasificación óptima se realiza maximizando el margen de separación entre las clases. Los vectores que definen el borde de esta separación son los vectores de soporte [@utkin2019imprecise]. 

A continuación se procede a mostrar información de este algoritmo, cómo se implementa y los valores de sus hiperparámetros por defecto:

```{r}
# Información sobre algoritmo `regLogistic`
getModelInfo("svmRadial")$svmRadial$grid
```
Como se puede apreciar en la ejecución anterior, se muestran los valores por defecto que toman los hiperparámetros de este algoritmo ((`sigma = mean(as.vector(sigmas[-2])), C = 2^((1:len) - 3`)) así como la forma de implementación del mismo.

Posee 2 hiperparámetros:

* `sigma`: coeficiente del kernel radial.
* `C`: penalización por violaciones del margen del hiperplano.

```{r}
# Hiperparámetros del algoritmo `svmRadial` de Máquina de Vector Soporte
maquina_vectores_info <- getModelInfo("svmRadial")
knitr::kable(maquina_vectores_info$svmRadial$parameters, caption="Hiperparámetros del algoritmo `svmRadial` de Máquina de Vector Soporte") %>% kable_styling(bootstrap_options = "condensed")
```

Se utilizará el método `cv` como se ha hecho en todos los modelos anteriores y se especificará un valor de `tuneLength` = 10, como se hizo previamente en el modelo anterior. También se especificará que tome como métrica "*Accuracy*":

```{r}
# SVM con datos balanceados con downSample
cl <- makeCluster(5)
registerDoParallel(cl)

set.seed(123)

svm_control_train <- trainControl(method = "cv", number = 10,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

set.seed(123)
if (file.exists("svm_downSample")) {
  svm_downSample <- readRDS("svm_downSample")
} else {
  svm_downSample <- caret::train(Class ~ ., data = datos3UTRtrain_down, method = "svmRadial", 
                                       tuneLength = 5, metric = "Accuracy", trControl = svm_control_train)
  saveRDS(svm_downSample, "svm_downSample")
}

# Resultados:
svm_downSample
```

A continuación se va a proceder a visualizar el modelo final para el caso de SVM con datos balanceados con downSample, tanto estadísticamente como gráficamente:

```{r}
# Modelo final de SVM para datos balanceados con downSample 
svm_downSample$finalModel
```

```{r}
# Representación gráfica del modelo SVM para datos balanceados con downSample
ggplot(svm_downSample, highlight = TRUE) +
  labs(title = "Evolución del accuracy del modelo SVM para datos balanceados con downSample") +
  theme_classic()
```

Como se puede apreciar, se obtiene una *Accuracy* del 94% y un valor de *Kappa* del 88%, por lo que da unos muy buenos resultados para el caso de los datos balanceados con downSample. 

En cuanto al modelo final, el mejor ajuste de los hiperparámetros para el mejor modelo es: `sigma = 0.0383442930737722` y `C = 4`.

Aunque este es de los modelos descartados el que mejor valor de accuracy posee junto a `rf`, sigue siendo mejor lo reflejado en el Ranger forest y en el Deep learning, por lo que se decidió descartarlo y no proceder a su comparación.

# Discusión

## Comparación Ranger forest y deep learning usando datos balanceados con `downSample` y `upSample`

De la comparación de los modelos Ranger forest (algoritmo `ranger`) y Deep learning usando el algoritmo `mlpKerasDropout` usando los métodos de balanceo de datos `downSample` y `upSample` se puede extraer que, **en este caso particular y en este estudio, los modelos dan unos mejores resultados si se utiliza para balancear los datos la estrategia de `upSample`, es decir, equiparar la clase minoritaria a la mayoritaria,** consiguiéndose un mayor número de muestras.

Dependiendo del tipo de estudio y de los datos que ofrezca el mismo, será conveniente seguir una estrategia de balanceo u otra, pero esa es una decisión que el autor del estudio debe tomar de forma temprana, por lo que hay que asumir en ocasiones que quizá el proceso no esté del todo optimizado.

En este caso se decidió implementar los modelos de Ranger forest y Deep learning con los dos tipos de datos balanceados, con estos dos tipos de estrategia, y el resto de modelos sólo con la estrategia de balanceo `downSample`, ya que se consideró que debido a la reducción del tiempo de cómputo que se produce al verse reducidas drásticamente las muestras, y a que los resultados no parecen ser sustancialmente peores siguiendo esta estrategia de balanceo, merecía la pena probar el resto de los modelos usando esta estrategia.

## Comparación de Ranger forest y deep learning usando datos balanceados `downSample`

De la comparación de los modelos Ranger forest (algoritmo `ranger`) y Deep learning usando el algoritmo `mlpKerasDropout` usando los métodos de balanceo de datos `downSample` se extrae que **el mejor modelo para predecir regiones 3-UTR es el Ranger forest para los datos de este estuduo y este caso particular**.

Este modelo resulta muy bueno discriminando regiones 3-UTR de las regiones que no lo son, y es el que mejores resultados ha dado de todos los modelos implementados en este estudio.

## Predictores importantes para la identificación de regiones 3'UTR

En los datos del estudio se contaba con una serie de predictores que iban a ayudar a la identificación de las regiones 3'UTR y a identificar aquellas que no lo son.

A priori, desde el punto de vista biológico, se podía preveer que algunos de estos predictores iban a ser importantes en su identificación, como es el predictor señal polyA. Este predictor resulta importante porque está seguido de de las regiones 3'UTR en el genoma, como se ve en la Figura a continuación, por lo que era de suponer que iba a tener una gran implicación en la detección de estas regiones y en la diferenciación de las mismas de las que no son regiones 3'UTR.

Puesto que los sitios polyA están presentes al final de las regiones 3'UTR, las 3'UTR no anotadas cuando sí deberían haberse anotado deben superponerse o estar cerca de estos sitios polyA.

![Estructura típica del mRNA humano](figura2.png)

Tras la **comprobación de la implicación de los predictores en su importancia a la hora de discernir regiones 3-UTR de las que no lo son**, se encontró que los predictores más importantes en este caso particular y con estos datos son: `polyA_signal1`, `meanPd`, `meanEE` y `mean_phastCons7way` para el modelo de `rf`, un modelo con resultados casi idénticos que el `ranger`, que resultó ser el mejor modelo para predecir en este estudio.

Es de esperar que la PRESENCIA de la secuencia poliA (por eso se refleja un "1" en el nombre del predictor, para determinar su presencia) se encuentre en la primera posición en cuanto a predictores más importantes para identificar regiones 3-UTR por la posición que ocupa esta secuencia y lo que se ha explicado previamente. Esto explica el por qué de su valor tan alto de predicción para las regiones 3-UTR (100%). 

Tras `polyA_signal1`, se encuentran `meanPD` y `meanEE` como los predictores más importantes, por lo que se puede decir que el modelo usa estas características genómicas y transcriptómicas para clasificar las regiones 3'UTR [@sethi2021leveraging].

El cuarto mejor predictor es `mean_phastCons7way`, la media de la estimación de la probabilidad de que cada nucleótido pertenezca a un elemento conservado, basándose en el alineamiento múltiple, y en un complejo terciario, en este caso.

# Conclusiones

* Es posible identificar regiones 3-UTR en el genoma humano con la implementación de los modelos comparados en el estudio.
* Todos los modelos implementados en este estudio han logrado un muy buen resultado en la predicción de regiones 3-UTR y en discernir aquellas que no lo son, pero los modelos más destacados son los modelos basados en árboles de decisión.
* Los datos del estudio y el preprocesamiento de los datos ha hecho posible que de los resultados de los modelos se obtengan valores muy buenos de precisión y de predicción.
* Las *features* o características más relevantes de los datos del estudio a la hora de indentificar regiones 3-UTR tienen implicaciones biológicas que corresponden a la realidad teórica.


# Referencias 
